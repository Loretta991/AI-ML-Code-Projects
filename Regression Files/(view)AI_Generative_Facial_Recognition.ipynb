{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":500,"status":"ok","timestamp":1698306903036,"user":{"displayName":"Loretta","userId":"14548570598164544149"},"user_tz":420},"id":"VUmCAVQlPWo-","outputId":"687bf194-419e-4518-94b3-79ccfbc535ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved: IMG_1947.3gp\n","Saved: image_with_squares2.jpeg\n","Saved: IMG-Singer1.jpg\n","Saved: image_with_squares1.jpeg\n","Saved: recorded_script_audio.mp4\n","Saved: scroll_both.txt\n","Saved: scroll_image2.txt\n","Saved: image_with_squares (1).jpeg\n","Saved: when_you_believe_audio.mp3\n","Saved: IMG-1760.jpg\n","Saved: scroll_image1.txt\n","Saved: IMG-Singer2.jpg\n","Files saved in the current directory: ['.config', 'shape_predictor_68_face_landmarks.dat', 'IMG_1947.3gp', 'dlib_face_recognition_resnet_model_v1.dat.bz2', 'image_with_squares2.jpeg', 'IMG-Singer1.jpg', 'final_output.mp4', '__temp__.png', 'dlib_face_recognition_resnet_model_v1.dat', 'image_with_squares1.jpeg', 'recorded_script_audio.mp4', 'scroll_both.txt', '.ipynb_checkpoints', 'scroll_image2.txt', 'image_with_squares (1).jpeg', 'sample_data', 'when_you_believe_audio.mp3', 'IMG-1760.jpg', 'scroll_image1.txt', 'shape_predictor_68_face_landmarks.dat.bz2', 'IMG-Singer2.jpg']\n"]}],"source":["# Run this code to save all uploaded files from the sample_data directory to the current directory\n","\n","import os\n","import shutil\n","\n","# Set the source directory where your uploaded files are located\n","upload_dir = '/content/sample_data'  # Update this path to point to your files\n","\n","# Get the current directory where your Jupyter Notebook is located\n","current_dir = os.getcwd()\n","\n","# Iterate through files in the upload directory and copy them to the current directory\n","for file in os.listdir(upload_dir):\n","    if os.path.isfile(os.path.join(upload_dir, file)):\n","        # Copy the file and display a message\n","        source_file = os.path.join(upload_dir, file)\n","        destination_file = os.path.join(current_dir, file)\n","        shutil.copy(source_file, destination_file)\n","        print(f\"Saved: {file}\")\n","\n","# List all the files that have been copied to the current directory\n","print(\"Files saved in the current directory:\", os.listdir(current_dir))\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1kMZGq0JKfotVmugKJXNnr5atUduGobEj","height":1000},"executionInfo":{"elapsed":462913,"status":"ok","timestamp":1698307620117,"user":{"displayName":"Loretta","userId":"14548570598164544149"},"user_tz":420},"id":"Oxum2PQ4PezJ","outputId":"3a982676-f10c-4e52-9748-4e80abc6e33f"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#RUN THS SNIPPET FIRST DOWNLOAD MODEL FOR FACE LANDMARKS FROM GITHUB\n","!pip install dlib opencv-python-headless imutils matplotl\n","\n","import dlib\n","import urllib.request\n","import bz2\n","import os\n","\n","# Define the URL of the compressed model file on GitHub\n","model_url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2\"\n","\n","# Define the local file path where you want to save the model\n","model_path = \"shape_predictor_68_face_landmarks.dat\"\n","\n","# Check if the model file already exists, if not, download and extract it\n","if not os.path.exists(model_path):\n","    print(\"Downloading and extracting the model...\")\n","    urllib.request.urlretrieve(model_url, model_path + \".bz2\")\n","    with bz2.BZ2File(model_path + \".bz2\", \"rb\") as source, open(model_path, \"wb\") as dest:\n","        dest.write(source.read())\n","    print(\"Model downloaded and extracted successfully!\")\n","\n","# Load the pre-trained shape predictor model for facial landmarks\n","predictor = dlib.shape_predictor(model_path)\n","\n","#------------------------------------------------------------\n","\n","#RUN THIS SNIPPET NEXT TO GET DLIB AND OTHER LIBRARIES\n","# Install dlib without uninstalling it and without CUDA support\n","#!pip install dlib --no-cache-dir --force-reinstall\n","import os\n","\n","# Force dlib to use CPU (hide GPUs)\n","os.environ['CUDA_VISIBLE_DEVICES'] = ''\n","\n","# Import other necessary libraries\n","import dlib\n","# Import other libraries and modules you need for your project\n","\n","import urllib.request\n","import bz2\n","import os\n","\n","# Replace the model_path variable with the specific path to the model file\n","model_path = \"/content/dlib_face_recognition_resnet_model_v1.dat\"\n","\n","# Check if the model file already exists, if not, download and extract it\n","if not os.path.exists(model_path):\n","    print(\"Downloading and extracting the model...\")\n","    urllib.request.urlretrieve(model_url, model_path + \".bz2\")\n","    with bz2.BZ2File(model_path + \".bz2\", \"rb\") as source, open(model_path, \"wb\") as dest:\n","        dest.write(source.read())\n","    print(\"Model downloaded and extracted successfully!\")\n","\n","# Load the pre-trained face recognition model\n","facerec = '/content/dlib_face_recognition_resnet_model_v1.dat'\n","\n","#------------------------------------------------------------------------\n","\n","#RUN THIS PREDICTOR MODEL FOR FACIAL LANDMARKS\n","\n","!pip install --upgrade pip\n","# First, install the necessary libraries\n","!pip install dlib opencv-python-headless imutils moviepy\n","!pip install opencv-python-headless\n","!pip install moviepy\n","!pip install ffmpeg-python\n","\n","import cv2\n","import dlib\n","import numpy as np\n","import moviepy.editor as mp\n","import time\n","\n","from IPython.display import Audio, display, Image\n","from moviepy.editor import AudioFileClip\n","from moviepy.editor import VideoFileClip\n","from moviepy.editor import ImageClip\n","from moviepy.editor import CompositeVideoClip\n","from moviepy.editor import VideoFileClip, CompositeVideoClip, clips_array\n","from moviepy.editor import AudioFileClip\n","\n","#----------------------------------------------------------------\n","\n","# Load pre-trained shape predictor model for facial landmarks\n","shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n","predictor = dlib.shape_predictor(shape_predictor_path)\n","\n","# Load pre-trained deep learning model for face detection\n","face_rec_model_path = 'dlib_face_recognition_resnet_model_v1.dat'\n","detector = dlib.get_frontal_face_detector()\n","\n","# Load the song\n","song_url = \"/content/sample_data/when_you_believe_audio.mp3\"\n","song_clip = mp.AudioFileClip(song_url)\n","song_duration = 110  # 1:50 (110 seconds)\n","\n","# Load the input JPEG images\n","input_image_path1 = '/content/sample_data/IMG-Singer1.jpg'\n","image1 = cv2.imread(input_image_path1)\n","gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","\n","# Load the input JPEG images\n","input_image_path2 = '/content/sample_data/IMG-Singer2.jpg'\n","image2 = cv2.imread(input_image_path2)\n","gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","\n","# Detect faces in the input images\n","faces1 = detector(gray1)\n","faces2 = detector(gray2)\n","\n","# Assuming there's only one face detected in each image\n","if len(faces1) > 0 and len(faces2) > 0:\n","    face1 = faces1[0]\n","    face2 = faces2[0]\n","\n","    # Get facial landmarks for the first image\n","    landmarks1 = predictor(gray1, face1)\n","    mouth_coords1 = [(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(48, 68)]\n","\n","    # Get facial landmarks for the second image\n","    landmarks2 = predictor(gray2, face2)\n","    mouth_coords2 = [(landmarks2.part(i).x, landmarks2.part(i).y) for i in range(48, 68)]\n","\n","    # Define mouth region coordinates for the first image\n","    x1, y1, w1, h1 = cv2.boundingRect(np.array(mouth_coords1))\n","\n","    # Define mouth region coordinates for the second image\n","    x2, y2, w2, h2 = cv2.boundingRect(np.array(mouth_coords2))\n","\n","    # Draw a green square around the detected face in the first image\n","    cv2.rectangle(image1, (face1.left(), face1.top()), (face1.right(), face1.bottom()), (0, 255, 0), 2)\n","\n","    # Draw a red square around the mouth in the first image\n","    cv2.rectangle(image1, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 255), 2)\n","\n","    # Draw a green square around the detected face in the second image\n","    cv2.rectangle(image2, (face2.left(), face2.top()), (face2.right(), face2.bottom()), (0, 255, 0), 2)\n","\n","    # Draw a red square around the mouth in the second image\n","    cv2.rectangle(image2, (x2, y2), (x2 + w2, y2 + h2), (0, 0, 255), 2)\n","\n","    # Display the first image with squares and add an audio button\n","    image_with_squares_path1 = '/content/sample_data/image_with_squares1.jpeg'\n","    cv2.imwrite(image_with_squares_path1, image1)\n","    display(Image(filename=image_with_squares_path1, width=400))\n","    display(Audio(filename='/content/sample_data/when_you_believe_audio.mp3'))\n","\n","    # Display the second image with squares and add an audio button\n","    image_with_squares_path2 = '/content/sample_data/image_with_squares2.jpeg'\n","    cv2.imwrite(image_with_squares_path2, image2)\n","    display(Image(filename=image_with_squares_path2, width=400))\n","    display(Audio(filename='/content/sample_data/when_you_believe_audio.mp3'))\n","else:\n","    print(\"No face detected in one or both input images after trying multiple rotations.\")\n","\n","#-----------------------------------------------------------------\n","\n","# Use OpenCV to detect and track mouth movements\n","\n","# Load the .jpg images for overlay\n","image1 = mp.ImageClip(\"/content/sample_data/IMG-Singer1.jpg\")\n","image2 = mp.ImageClip(\"/content/sample_data/IMG-Singer2.jpg\")\n","\n","\n","# Load the mouth movement videos\n","video1 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","#video2 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","\n","# Set the duration for overlay (1:50 - 0:33 seconds)\n","overlay_duration1 = song_duration - 33\n","\n","# Set the duration for overlay (2:27 - 0:33 seconds)\n","#overlay_duration2 = song_duration - 33\n","\n","# Overlay the words over the images\n","overlay1 = mp.CompositeVideoClip([image1.set_duration(overlay_duration1)])\n","#overlay2 = mp.CompositeVideoClip([image2.set_duration(overlay_duration2)])\n","\n","# Create the script audio\n","script_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","\n","# Combine images and script audio\n","final_clip1 = mp.clips_array([[overlay1]])\n","final_clip1 = final_clip1.set_audio(script_audio)\n","\n","# Add an audio button to play the song\n","song_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","final_clip1 = final_clip1.set_audio(song_audio)\n","\n","# Set the duration for the images to match the videos\n","image1 = image1.set_duration(video1.duration)\n","\n","# Create a video clip with the images side by side\n","side_by_side = clips_array([[image1, image2]])\n","\n","# Overlay the mouth movement videos on the images\n","video1 = video1.set_position(\"center\")\n","#video2 = video2.set_position(\"center\")\n","\n","#final_clip = CompositeVideoClip([side_by_side, video1, video2])\n","\n","# Overlay the mouth movement video on top of a black background\n","mouth_movement_overlay1 = video1.set_position(\"center\").on_color(\n","    (1920, 1080), color=(0, 0, 0), pos=\"center\")\n","\n","# Combine the mouth movement overlay with the audio\n","final_clip1 = CompositeVideoClip([mouth_movement_overlay1.set_audio(song_audio)])\n","\n","# Write the final video\n","final_clip1.write_videofile(\"final_output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n","\n","print('Final output video printed')\n","# Display the audio playback button for the song\n","final_clip1.ipython_display(t=0, width=800)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvZbmrBAVYc5mthPkp2C+L"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
