{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP3D1QgKSgnM42x/uxh3GaQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"rbF8Hi6utKzk"},"outputs":[],"source":["\n","\"\"\"\n","Data below not used in the Covid 19 Project\n","# Let's summarize the data to see the distribution of data\n","print(data.describe())\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from scipy import stats\n","\n","#Columns like TOT_DEATH, TOT_CASES, CONF_CASES, NEW_CASES,STATE seems to have outliers. \n","#Let's see the outliers percentage in these columns.\n","\n","for k, v in data.items():\n","        q1 = v.quantile(0.25)\n","        q3 = v.quantile(0.75)\n","        irq = q3 - q1\n","        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n","        perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n","        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n","\n","    \n","data= data[~(data['tot_cases'] >= 50.0)]\n","print(np.shape(data))\n","\n","\n","fig, axs = plt.subplots(ncols=5, nrows=1, figsize=(20, 10))\n","index = 0\n","\n","axs = axs.flatten()\n","for k,v in data.items():\n","    sns.distplot(v, ax=axs[index])\n","    index += 1\n","plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n","\n","\n","plt.figure(figsize=(20, 10))\n","sns.heatmap(data.corr().abs(),  annot=True)\n"," \n","\n","from sklearn import preprocessing\n","# Let's scale the columns before plotting them against tot_cases\n","\n","min_max_scaler = preprocessing.MinMaxScaler()\n","column_sels = ['tot_death','tot_cases','conf_cases', 'prob_cases', 'new_case']\n","x = data.loc[:,column_sels]\n","y = data['tot_cases']\n","x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n","\n","\n","ig, axs = plt.subplots(ncols=4, nrows=1, figsize=(10,10)\n","\n","\n","axs = axs.flatten()\n","\n","for i, k in enumerate(column_sels):\n","    sns.regplot(y=y, x=x[k], ax=axs[i])\n","plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n","\n","#o with these analsis, we may try predict MEDV with ''tot_death','tot_cases','conf_cases', 'prob_cases', 'new_case' \\\n","#features. Let's try to remove the skewness of the data trough log transformation\n","\n"," \n","y =  np.log1p(y)\n","for col in x.columns:\n","    if np.abs(x[col].skew()) > 0.3:\n","        x[col] = np.log1p(x[col])\n"," \n","#From correlation matrix, we see TOT_CASES and TOT_DEATH are highly correlated features. \\\n","#The columns 'conf_cases', 'prob_cases', 'new_case' has a correlation score above 0.5 \\\n","##with MEDV which is a good indication of using as predictors. \n","\n","#add Codeadd Markdown  \n","\n","#The Liner Regression with and without L2 regularization does not make significant difference \\\n","#is MSE score. However polynomial regression with degree=3 has a better MSE. Let's try some non \\\n","#prametric regression techniques: SVR with kernal rbf, DecisionTreeRegressor, KNeighborsRegressor et\n","\n","from sklearn.svm import SVR\n","from sklearn.model_selection import GridSearchCV\n","\n","sVr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n","#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n","#grid_sv.fit(x_scaled, y)\n","#print(\"Best classifier :\", grid_sv.best_estimator_)\n","\n","scores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n","scores_map['SVR'] = scores\n","print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","\n","from sklearn.tree import DecisionTreeRegressor\n","\n","desc_tr = DecisionTreeRegressor(max_depth=5)\n","#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n","#grid_sv.fit(x_scaled, y)\n","#print(\"Best classifier :\", grid_sv.best_estimator_)\n","\n","scores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n","scores_map['DecisionTreeRegressor'] = scores\n","print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","knn = KNeighborsRegressor(n_neighbors=7)\n","scores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n","scores_map['KNeighborsRegressor'] = scores\n","\n","\n","#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n","#grid_sv.fit(x_scaled, y)\n","#print(\"Best classifier :\", grid_sv.best_estimator_)\n","print(\"KNN Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","\n","\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n","#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n","#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n","#grid_sv.fit(x_scaled, y)\n","#print(\"Best classifier :\", grid_sv.best_estimator_)\n","\n","#scores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n","#scores_map['GradientBoostingRegressor'] = scores\n","print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n","\"\"\""]}]}