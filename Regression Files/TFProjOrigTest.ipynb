{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxNYBLZK2b6zFoHQUQD4lK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nr5My-qN8edU"},"outputs":[],"source":["\n","#section 1\n","import os\n","import urllib.request\n","import zipfile\n","\n","# Set the URLs for the COCO 2014 dataset zip files\n","train_url = 'http://images.cocodataset.org/zips/train2014.zip'\n","val_url = 'http://images.cocodataset.org/zips/val2014.zip'\n","test_url = 'http://images.cocodataset.org/zips/test2014.zip'\n","train_annot_url = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n","test_annot_url = 'http://images.cocodataset.org/annotations/image_info_test2014.zip'\n","val_annot_url = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n","\n","\n","# Set the local directory to download and extract the dataset\n","data_dir = 'coco_dataset'\n","\n","# Download and extract the train dataset zip file\n","if not os.path.exists(data_dir):\n","    os.makedirs(data_dir)\n","if not os.path.exists(os.path.join(data_dir, 'train2014')):\n","    print('Downloading COCO 2014 train dataset...')\n","    urllib.request.urlretrieve(train_url, os.path.join(data_dir, 'train2014.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'train2014.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir, 'train2014.zip'))\n","    print('Done!')\n","else:\n","    print('COCO 2014 train dataset already downloaded and extracted.')\n","\n","# Download and extract the val dataset zip file\n","if not os.path.exists(os.path.join(data_dir, 'val2014')):\n","    print('Downloading COCO 2014 val dataset...')\n","    urllib.request.urlretrieve(val_url, os.path.join(data_dir, 'val2014.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'val2014.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir, 'val2014.zip'))\n","    print('Done!')\n","else:\n","    print('COCO 2014 val dataset already downloaded and extracted.')\n","\n","# Download and extract the test dataset zip file\n","if not os.path.exists(os.path.join(data_dir, 'test2014')):\n","    print('Downloading COCO 2014 test dataset...')\n","    urllib.request.urlretrieve(test_url, os.path.join(data_dir, 'test2014.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'test2014.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir, 'test2014.zip'))\n","    print('Done!')\n","else:\n","    print('COCO 2014 test dataset already downloaded and extracted.')\n","\n","# Download and extract the train annotation dataset zip file\n","if not os.path.exists(os.path.join(data_dir, 'annotations_trainval2014')):\n","    print('Downloading COCO 2014 train annotation dataset...')\n","    urllib.request.urlretrieve(train_annot_url, os.path.join(data_dir, 'trainval_annotations.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'trainval_annotations.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir, 'trainval_annotations.zip'))\n","    print('Done!')\n","else:\n","    print('COCO 2014 train annotation dataset already downloaded and extracted.')\n","\n","# Download and extract the validation annotation dataset zip file\n","if not os.path.exists(os.path.join(data_dir, 'annotations_val')):\n","    print('Downloading COCO 2014 val annotation dataset...')\n","    urllib.request.urlretrieve(val_annot_url, os.path.join(data_dir, 'annotations_val.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'annotations_val.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir, 'annotations_val.zip'))\n","    print('Done!')\n","else:\n","    print('COCO 2014 val annotation dataset already downloaded and extracted.')\n","\n","# Load the COCO annotations for the train, val, and test datasets\n","train_ann_path = os.path.join(data_dir, '/content/coco_dataset/annotations', 'instances_train2014.json')\n","\n","print(train_ann_path)\n","print(os.path.exists(train_ann_path))\n","\n","if not os.path.exists(os.path.join(data_dir, 'test2014')):\n","    print('Downloading COCO 2014 test dataset...')\n","    urllib.request.urlretrieve(test_url, os.path.join(data_dir, 'test2014.zip'))\n","    with zipfile.ZipFile(os.path.join(data_dir, 'test2014.zip'), 'r') as zip_ref:\n","        zip_ref.extractall(data_dir)\n","    os.remove(os.path.join(data_dir,'/content/coco_dataset/annotations', 'test2014.zip'))\n","val_ann_path = os.path.join(data_dir, '/content/coco_dataset/annotations', 'instances_val2014.json')\n","#test_ann_path = os.path.join(data_dir, '/content/coco_dataset/annotations', 'image_info_test2014.json')\n","test_ann_path = os.path.join(data_dir, '/content/coco_dataset/annotations/annotations', 'image_info_test2014.json')\n","\n","\"\"\"\n","train_ann= COCO(train_ann_path)\n","val_ann = COCO(val_ann_path)\n","test_ann = COCO(test_ann_path)\n","\"\"\"\n","train_ann= '/content/coco_dataset/train2014(train_ann_path)'\n","val_ann = '/content/coco_dataset/val2014(val_ann_path)' \n","test_ann = '/content/coco_dataset/test2014(test_ann_path)'\n","print('COCO annotations loaded successfully.')\n","\n","train_images_dir = '/content/coco_dataset'\n","train_annotations_file = '/content/coco_dataset/train2014'\n","\n","val_images_dir = '/content/coco_dataset/val2014'\n","val_annotations_file = 'path/to/val/annotations.csv'\n","\n","test_images_dir = '/content/coco_dataset/test2014'\n","test_annotations_file = '/content/coco_dataset/annotations/annotations'\n","\n","\n","\n","#another ssection \n","\"\"\"\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import cv2\n","import zipfile\n","import os\n","\n","# Download and extract the COCO dataset\n","url = \"http://images.cocodataset.org/zips/train2014.zip\"\n","zip_file = tf.keras.utils.get_file(\"train2014.zip\", url, extract=False)\n","with zipfile.ZipFile(zip_file, \"r\") as f:\n","    f.extractall()\n","\n","# Define the model architecture\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n","    tf.keras.layers.MaxPooling2D((2, 2)),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D((2, 2)),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(64, activation='relu'),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam',\n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Define the image preprocessing function\n","def preprocess_image(image_path):\n","    # Load image from file\n","    image = cv2.imread(image_path)\n","    \n","    # Resize the image to a fixed size\n","    resized_image = cv2.resize(image, (224, 224))\n","    \n","    # Convert the image to RGB format\n","    rgb_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n","    \n","    # Normalize the image pixel values\n","    normalized_image = rgb_image / 255.0\n","    \n","    # Return the preprocessed image\n","    return normalized_image\n","\n","# Define the data generator function\n","def data_generator(batch_size):\n","    # Load the image filenames and annotations from the COCO dataset\n","    image_dir = \"train2014\"\n","    annotations_file = \"annotations/instances_train2014.json\"\n","    coco = COCO(annotations_file)\n","    image_ids = coco.getImgIds()\n","    images = coco.loadImgs(image_ids)\n","    annotations = coco.loadAnns(coco.getAnnIds(image_ids))\n","    \n","    # Loop over the images and annotations and yield batches of data\n","    while True:\n","        batch_images = []\n","        batch_labels = []\n","        for i in range(batch_size):\n","            # Select a random image and its annotations\n","            image = np.random.choice(images)\n","            image_path = os.path.join(image_dir, image['file_name'])\n","            image_annotations = [ann for ann in annotations if ann['image_id'] == image['id']]\n","            \n","            # Preprocess the image\n","            preprocessed_image = preprocess_image(image_path)\n","            \n","            # Create the label tensor\n","            label_tensor = create_label_tensor(image_annotations)\n","            \n","            # Add the preprocessed image and label tensor to the batch\n","            batch_images.append(preprocessed_image)\n","            batch_labels.append(label_tensor)\n","        \n","        # Yield the batch of data\n","        yield np.array(batch_images), np.array(batch_labels)\n","# Define the label creation function\n","def create_label_tensor(annotations):\n","    # Create an empty label tensor\n","    label_tensor = np.zeros((224, 224, 10))\n","    \n","    # Loop over the annotations and fill in the label tensor\n","    for ann in annotations:\n","        category_id = ann['category_id']\n","        category_index = category_id - 1\n","        x, y, w, h = ann['bbox']\n","        x = int(round(x))\n","        y = int(round(y))\n","        w = int(round(w))\n","        h = int(round(h))\n","        x_center = x + w / 2\n","        y_center = y + h / 2\n","        x_center /= 224\n","        y_center /= 224\n","        w /= 224\n","        h /= 224\n","        label_tensor[y:y+h, x:x+w, category_index] = 1.0\n","        label_tensor[y:y+h, x:x+w, 4:8] = np.array([x_center, y_center, w, h])\n","        \n","    return label_tensor\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","# Load the validation data\n","val_data = (\"/content/coco_dataset/val2014\")\n","\n","# Create a validation data generator\n","def val_generator(batch_size):\n","    while True:\n","        for i in range(0, len(val_data['images']), batch_size):\n","            images = []\n","            labels = []\n","            for image_data in val_data['images'][i:i+batch_size]:\n","                image = load_image(image_data['file_name'])\n","                annotations = get_annotations(image_data['id'], val_data['annotations'])\n","                label_tensor = create_label_tensor(annotations)\n","                images.append(image)\n","                labels.append(label_tensor)\n","            images = np.array(images)\n","            labels = np.array(labels)\n","            yield images, labels\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Define the sigmoid function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Generate some random data\n","data = np.random.randn(1000, 10)\n","\n","# Compute the correlation matrix\n","corr = np.corrcoef(data, rowvar=False)\n","\n","# Plot the sigmoid function\n","x = np.linspace(-10, 10, 1000)\n","y = sigmoid(x)\n","plt.plot(x, y)\n","plt.xlabel('Input')\n","plt.ylabel('Output')\n","plt.title('Sigmoid Activation Function')\n","plt.show()\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","# Plot the correlation matrix\n","sns.set(style='white')\n","mask = np.triu(np.ones_like(corr, dtype=np.bool))\n","cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","plt.title('Correlation Matrix')\n","plt.show()\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","dummy_matrix = np.random.rand(10, 10)\n","plt.imshow(dummy_matrix, cmap='jet')\n","plt.colorbar()\n","plt.title(' Correlation Matrix')\n","plt.show()\n","\"\"\"\n","\n","\n","#another ssection \n","\"\"\"\n","import os\n","import numpy as np\n","from pycocotools.coco import COCO \n","\n","\n","# Define file paths\n","dataDir = '/content/coco_dataset'\n","train_annFile = os.path.join(dataDir, 'annotations', 'instances_train2014.json')\n","val_annFile = os.path.join(dataDir, 'annotations', 'instances_val2014.json')\n","test_annFile = os.path.join(dataDir, 'annotations', 'instances_test2014.json')\n","\n","# Load COCO annotations\n","train_coco = COCO(train_annFile)\n","val_coco = COCO(val_annFile)\n","test_coco = ('/content/coco_dataset/test2014(test_annFile')\n","\n","# Get all category ids\n","category_ids = sorted(train_coco.getCatIds())\n","\n","# Get all image ids for training, validation, and testing sets\n","train_img_ids = sorted(train_coco.getImgIds())\n","val_img_ids = sorted(val_coco.getImgIds())\n","#test_img_ids = sorted(test_coco.getImgIds())\n","\n","# Create a dictionary to map category names to their ids\n","category_id_to_name_map = {}\n","for cat in train_coco.loadCats(category_ids):\n","    category_id_to_name_map[cat['id']] = cat['name']\n","\n","# Print the number of images and categories for each set\n","print(f\"Number of images in training set: {len(train_img_ids)}\")\n","print(f\"Number of categories in training set: {len(category_ids)}\")\n","print(f\"Number of images in validation set: {len(val_img_ids)}\")\n","print(f\"Number of categories in validation set: {len(category_ids)}\")\n","#print(f\"Number of images in testing set: {len(test_img_ids)}\")\n","print(f\"Number of categories in testing set: {len(category_ids)}\")\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","from tensorflow.keras.layers import (\n","    Input,\n","    Conv2D,\n","    MaxPooling2D,\n","    Dense,\n","    Flatten,\n","    GlobalAveragePooling2D,\n","    BatchNormalization,\n","    LeakyReLU,\n",")\n","from tensorflow.keras.applications import ResNet50V2\n","from pycocotools.coco import COCO\n","\n","\n","# Define the YOLO model\n","def create_yolo_model(input_shape):\n","    inputs = Input(shape=input_shape)\n","    x = Conv2D(32, (3, 3), strides=(1, 1), padding='same')(inputs)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    x = Conv2D(64, (3, 3), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    x = Conv2D(128, (3, 3), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    x = Conv2D(256, (3, 3), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    x = Conv2D(512, (3, 3), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    x = Conv2D(1024, (3, 3), strides=(1, 1), padding='same')(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.1)(x)\n","\n","    x = GlobalAveragePooling2D()(x)\n","\n","    outputs = Dense(80 + 5)(x)\n","\n","    model = Model(inputs, outputs)\n","    return model\n","\n","\n","# Define the R-CNN model\n","def create_rcnn_model(input_shape):\n","    base_model = ResNet50V2(include_top=False, weights='imagenet', input_shape=input_shape)\n","\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","\n","    rcnn_bbox = Dense(4, activation='linear', name='rcnn_bbox')(x)\n","    rcnn_class = Dense(80, activation='softmax', name='rcnn_class')(x)\n","\n","    model = Model(inputs=base_model.input, outputs=[rcnn_bbox, rcnn_class])\n","    return model\n","\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","# Load COCO annotations\n","dataDir = '/content/coco_dataset'\n","train_annFile = os.path.join(dataDir, 'annotations', 'instances_train2014.json')\n","val_annFile = os.path.join(dataDir, 'annotations', 'instances_val2014.json')\n","\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import tensorflow as tf\n","import keras\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense\n","from keras.utils import to_categorical\n","import numpy as np\n","import pandas as pd\n","import os\n","import cv2\n","\n","# Define YOLO model\n","def create_yolo_model(input_shape=(224, 224, 3)):\n","    input_layer = Input(input_shape)\n","    # TODO: Add YOLO model layers\n","    output_layer = Dense(10, activation='softmax')(input_layer)\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    return model\n","\n","# Define R-CNN model\n","def create_rcnn_model(input_shape=(224, 224, 3)):\n","    input_layer = Input(input_shape)\n","    # TODO: Add R-CNN model layers\n","    output_layer = Dense(10, activation='softmax')(input_layer)\n","    model = Model(inputs=input_layer, outputs=output_layer)\n","    return model\n","\n","# Define train function\n","def train_model(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=10):\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","    model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=epochs)\n","\n","# Define test function\n","def test_model(model, X_test, y_test):\n","    test_loss, test_acc = model.evaluate(X_test, y_test)\n","    print('Test loss:', test_loss)\n","    print('Test accuracy:', test_acc)\n","\n","# Define val function\n","def val_model(model, X_val, y_val):\n","    val_loss, val_acc = model.evaluate(X_val, y_val)\n","    print('Validation loss:', val_loss)\n","    print('Validation accuracy:', val_acc)\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","# Load dataset\n","dataDir = '/path/to/coco/dataset'\n","dataType = 'train2014'\n","annFile='{}/annotations/instances_{}.json'.format(dataDir,dataType)\n","\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import os\n","import random\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","# Set path to dataset\n","dataset_dir = \"/content/coco_dataset/train2014\"\n","\n","# Get list of image files in dataset directory\n","img_files = os.listdir(dataset_dir)\n","\n","# Select 24 random image files\n","img_files = random.sample(img_files, 24)\n","\n","# Plot the selected images\n","plt.figure(figsize=(10, 10))\n","for i, img_file in enumerate(img_files):\n","    img_path = os.path.join(dataset_dir, img_file)\n","    img = Image.open(img_path)\n","    plt.subplot(4, 6, i+1)\n","    plt.imshow(img)\n","    plt.axis('off')\n","plt.show()\n","\n","\"\"\"\n","\n","#another ssection \n","\"\"\"\n","import os\n","import random\n","import cv2\n","from pycocotools.coco import COCO\n","from google.colab.patches import cv2_imshow\n","\n","# Path to COCO dataset directory\n","dataDir = 'coco_dataset'\n","dataType = 'train2014'\n","\n","# Initialize COCO API for annotations\n","annFile = os.path.join(dataDir, 'annotations', 'instances_{}.json'.format(dataType))\n","coco = COCO(annFile)\n","\n","# Define list of animal and person category IDs\n","animal_catIds = coco.getCatIds(catNms=['bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe'])\n","person_catIds = coco.getCatIds(catNms=['person'])\n","\n","# Get list of image IDs for the given category IDs\n","animal_imgIds = coco.getImgIds(catIds=animal_catIds)\n","person_imgIds = coco.getImgIds(catIds=person_catIds)\n","\n","# Choose 24 random images from each category\n","animal_imgIds = random.sample(animal_imgIds, min(len(animal_imgIds), 24))\n","person_imgIds = random.sample(person_imgIds, min(len(person_imgIds), 24))\n","\n","# Combine the two lists of image IDs\n","imgIds = animal_imgIds + person_imgIds\n","\n","# Loop through selected images and display them\n","for imgId in imgIds:\n","    # Load image\n","    imgInfo = coco.loadImgs(imgId)[0]\n","    imgFile = os.path.join(dataDir, dataType, imgInfo['file_name'])\n","    img = cv2.imread(imgFile)\n","\n","    # Apply saturation filter to enhance color\n","    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","    h, s, v = cv2.split(hsv)\n","    s = cv2.add(s, 50)\n","    hsv = cv2.merge((h, s, v))\n","    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","\n","    # Display image with category label\n","    annIds = coco.getAnnIds(imgIds=imgId, catIds=animal_catIds+person_catIds)\n","    anns = coco.loadAnns(annIds)\n","    catNames = [coco.loadCats(ann['category_id'])[0]['name'] for ann in anns]\n","    catNames = list(set(catNames))\n","    label = ', '.join(catNames)\n","    cv2.putText(img, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n","    cv2_imshow(img)\n","    cv2.waitKey(0)\n","\n","cv2.destroyAllWindows()\n","\"\"\"\n","#another ssection \n","\"\"\"\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pycocotools.coco import COCO\n","\n","# Path to COCO dataset directory\n","dataDir = 'coco_dataset'\n","dataType = 'train2014'\n","\n","# Initialize COCO API for annotations\n","annFile = os.path.join(dataDir, 'annotations', 'instances_{}.json'.format(dataType))\n","coco = COCO(annFile)\n","\n","# Get all annotations\n","annIds = coco.getAnnIds()\n","anns = coco.loadAnns(annIds)\n","\n","# Compute areas of all objects\n","areas = [ann['area'] for ann in anns]\n","\n","if not areas:\n","    print(\"No annotated objects found in COCO dataset\")\n","else:\n","    # Compute correlation matrix\n","    corr_matrix = np.corrcoef(areas)\n","\n","    # Plot the correlation matrix\n","    plt.imshow(corr_matrix, cmap='jet')\n","    plt.colorbar()\n","    plt.title('Correlation Matrix of Object Areas in COCO Dataset')\n","    plt.show()\n","\"\"\"\n","\n","\n"]}]}