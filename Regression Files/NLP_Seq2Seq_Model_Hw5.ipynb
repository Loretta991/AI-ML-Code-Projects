{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g12chi-q5r5t"},"outputs":[],"source":["#Module 1: Importing Libraries\n","!pip install --upgrade pip\n","!pip install tensorflow\n","import tensorflow as tf\n","# Or import torch if you're using PyTorch\n","!pip install torch\n","\n","import torch\n","# Other necessary imports\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7QlMDZg97SZY"},"outputs":[],"source":["# Module 2: Processsing Data\n","!pip install --upgrade pip\n","!pip install tensorflow\n","import tensorflow as tf \n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","def preprocess_data(input_data, output_data, input_vocab_size, output_vocab_size, input_seq_length, output_seq_length):\n","    # Tokenization\n","    input_tokens = tokenize_input_data(input_data)\n","    output_tokens = tokenize_output_data(output_data)\n","    \n","    # Padding\n","    padded_input = pad_sequences(input_tokens, maxlen=input_seq_length, padding='post')\n","    padded_output = pad_sequences(output_tokens, maxlen=output_seq_length, padding='post')\n","    \n","    # Convert to numerical representations\n","    input_numerical = numerical_representation(padded_input, input_vocab_size)\n","    output_numerical = numerical_representation(padded_output, output_vocab_size)\n","    output_categorical = to_categorical(output_numerical, num_classes=output_vocab_size)\n","    \n","    return input_numerical, output_categorical\n","\n","\n","def tokenize_input_data(input_data):\n","    # Tokenization logic for input data\n","    # Return a list of tokenized input sequences\n","    input_tokens = []\n","    for sequence in input_data:\n","        tokens = sequence.split()\n","        input_tokens.append(tokens)\n","    return input_tokens\n","\n","def tokenize_output_data(output_data):\n","    # Tokenization logic for output data\n","    # Return a list of tokenized output sequences\n","    output_tokens = []\n","    for sequence in output_data:\n","        tokens = sequence.split()\n","        output_tokens.append(tokens)\n","    return output_tokens\n","\n","def numerical_representation(tokens, vocab_size):\n","    # Convert tokens to numerical representations\n","    # Each token is represented by an integer index\n","    numerical = [[token_to_index(token) for token in sequence] for sequence in tokens]\n","    return np.array(numerical)\n","\n","def token_to_index(token):\n","    # Map tokens to integer indices\n","    # Return the corresponding index for the token\n","    return index\n","\n","# Example usage\n","input_data = ['I love NLP', 'Seq2Seq models are powerful']\n","output_data = ['Je aime NLP', 'Les modeles Seq2Seq sont puissants']\n","input_vocab_size = 10000\n","output_vocab_size = 10000\n","input_seq_length = 10\n","output_seq_length = 10\n","\n","preprocessed_input, preprocessed_output = preprocess_data(input_data, output_data, input_vocab_size, output_vocab_size, input_seq_length, output_seq_length)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":7247,"status":"error","timestamp":1685758765232,"user":{"displayName":"Loretta","userId":"14548570598164544149"},"user_tz":420},"id":"t8Bj8TvFDQ_N","outputId":"72147fdf-fb9a-48d3-dfea-278533e71462"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Requirement already satisfied: absl-py\u003e=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers\u003e=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n","Requirement already satisfied: gast\u003c=0.4.0,\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: grpcio\u003c2.0,\u003e=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n","Requirement already satisfied: h5py\u003e=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: jax\u003e=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n","Requirement already satisfied: keras\u003c2.13,\u003e=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: libclang\u003e=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n","Requirement already satisfied: numpy\u003c1.24,\u003e=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n","Requirement already satisfied: opt-einsum\u003e=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u003c5.0.0dev,\u003e=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six\u003e=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: tensorboard\u003c2.13,\u003e=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n","Requirement already satisfied: tensorflow-estimator\u003c2.13,\u003e=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions\u003e=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt\u003c1.15,\u003e=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem\u003e=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n","Requirement already satisfied: wheel\u003c1.0,\u003e=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse\u003e=1.6.0-\u003etensorflow) (0.40.0)\n","Requirement already satisfied: ml-dtypes\u003e=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax\u003e=0.3.15-\u003etensorflow) (0.1.0)\n","Requirement already satisfied: scipy\u003e=1.7 in /usr/local/lib/python3.10/dist-packages (from jax\u003e=0.3.15-\u003etensorflow) (1.10.1)\n","Requirement already satisfied: google-auth\u003c3,\u003e=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib\u003c1.1,\u003e=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (1.0.0)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (3.4.3)\n","Requirement already satisfied: requests\u003c3,\u003e=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server\u003c0.8.0,\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (0.7.0)\n","Requirement already satisfied: tensorboard-plugin-wit\u003e=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (1.8.1)\n","Requirement already satisfied: werkzeug\u003e=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2.3.0)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (5.3.0)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (0.3.0)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib\u003e=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (1.3.1)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (1.26.15)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2.0.12)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.21.0-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (3.4)\n","Requirement already satisfied: MarkupSafe\u003e=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug\u003e=1.0.1-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (2.1.2)\n","Requirement already satisfied: pyasn1\u003c0.6.0,\u003e=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth\u003c3,\u003e=1.6.3-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib\u003e=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib\u003e=0.7.0-\u003egoogle-auth-oauthlib\u003c1.1,\u003e=0.5-\u003etensorboard\u003c2.13,\u003e=2.12-\u003etensorflow) (3.2.2)\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-13-525233b53d22\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 63\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0moutput_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 63\u001b[0;31m \u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m\u003cipython-input-13-525233b53d22\u003e\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m(input_data, output_data, input_vocab_size, output_vocab_size, input_seq_length, output_seq_length)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 15\u001b[0;31m     \u001b[0mpadded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mpadded_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1129\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m             raise ValueError(\n","\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'I'"]}],"source":["!pip install --upgrade pip\n","!pip install tensorflow\n","\n","import numpy as np\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","def preprocess_data(input_data, output_data, input_vocab_size, output_vocab_size, input_seq_length, output_seq_length):\n","    # Tokenization\n","    input_tokens = tokenize_input_data(input_data)\n","    output_tokens = tokenize_output_data(output_data)\n","    \n","    # Padding\n","    padded_input = pad_sequences(input_tokens, maxlen=input_seq_length, padding='post')\n","    padded_output = pad_sequences(output_tokens, maxlen=output_seq_length, padding='post')\n","    \n","    # Convert to numerical representations\n","    input_numerical = numerical_representation(padded_input, input_vocab_size)\n","    output_numerical = numerical_representation(padded_output, output_vocab_size)\n","    output_categorical = to_categorical(output_numerical, num_classes=output_vocab_size)\n","    \n","    return input_numerical, output_categorical\n","\n","\n","def tokenize_input_data(input_data):\n","    # Tokenization logic for input data\n","    # Return a list of tokenized input sequences\n","    input_tokens = []\n","    for sequence in input_data:\n","        tokens = sequence.split()\n","        input_tokens.append(tokens)\n","    return input_tokens\n","\n","def tokenize_output_data(output_data):\n","    # Tokenization logic for output data\n","    # Return a list of tokenized output sequences\n","    output_tokens = []\n","    for sequence in output_data:\n","        tokens = sequence.split()\n","        output_tokens.append(tokens)\n","    return output_tokens\n","\n","def numerical_representation(tokens, vocab_size):\n","    # Convert tokens to numerical representations\n","    # Each token is represented by an integer index\n","    numerical = [[token_to_index(token) for token in sequence] for sequence in tokens]\n","    return np.array(numerical)\n","\n","def token_to_index(token):\n","    # Map tokens to integer indices\n","    # Return the corresponding index for the token\n","    return index\n","\n","# Example usage\n","input_data = ['I love NLP', 'Seq2Seq models are powerful']\n","output_data = ['Je aime NLP', 'Les modeles Seq2Seq sont puissants']\n","input_vocab_size = 10000\n","output_vocab_size = 10000\n","input_seq_length = 10\n","output_seq_length = 10\n","\n","preprocessed_input, preprocessed_output = preprocess_data(input_data, output_data, input_vocab_size, output_vocab_size, input_seq_length, output_seq_length)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQ5B1Ijv9Jqc"},"outputs":[],"source":["\n","!pip install --upgrade pip\n","import tensorflow as tf\n","import numpy as np\n","\n","# Define the alignment model\n","class AlignmentModel(tf.keras.Model):\n","    def __init__(self, hidden_size):\n","        super(AlignmentModel, self).__init__()\n","        self.W_a = tf.keras.layers.Dense(hidden_size)\n","        self.U_a = tf.keras.layers.Dense(hidden_size)\n","        self.v_a = tf.keras.layers.Dense(1)\n","\n","    def call(self, encoder_hidden_states, decoder_hidden_state):\n","        alignment_scores = self.v_a(\n","            tf.nn.tanh(self.W_a(encoder_hidden_states) + self.U_a(decoder_hidden_state))\n","        )\n","        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n","        return attention_weights\n","\n","# Create input sequences\n","input_seq = [\n","    ['word1', 'word2', 'word3', 'word4', 'word5'],\n","    ['word6', 'word7', 'word8', 'word9', 'word10', 'word11'],\n","    ['word12', 'word13', 'word14', 'word15', 'word16', 'word17', 'word18', 'word19', 'word20']\n","]\n","\n","# Convert words to indices\n","word_to_index = {}\n","index_to_word = {}\n","for sentence in input_seq:\n","    for word in sentence:\n","        if word not in word_to_index:\n","            word_to_index[word] = len(word_to_index)\n","            index_to_word[len(index_to_word)] = word\n","\n","# Convert input sequences to indices\n","input_indices = []\n","for sentence in input_seq:\n","    sentence_indices = [word_to_index[word] for word in sentence]\n","    input_indices.append(sentence_indices)\n","\n","# Pad sequences to have equal length\n","max_length = max(len(sentence) for sentence in input_indices)\n","padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n","    input_indices, maxlen=max_length, padding='post'\n",")\n","\n","# Build and compile the model\n","hidden_size = 32\n","encoder_inputs = tf.keras.layers.Input(shape=(max_length,))\n","decoder_hidden_state = tf.keras.layers.Input(shape=(hidden_size,))\n","encoder_hidden_states = tf.keras.layers.Embedding(len(word_to_index), hidden_size)(encoder_inputs)\n","attention_model = AlignmentModel(hidden_size)\n","attention_weights = attention_model(encoder_hidden_states, decoder_hidden_state)\n","attention_context = tf.reduce_sum(attention_weights * encoder_hidden_states, axis=1)\n","\n","model = tf.keras.Model(inputs=[encoder_inputs, decoder_hidden_state], outputs=attention_context)\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Generate random decoder hidden state\n","decoder_hidden_state = np.random.randn(hidden_size)\n","\n","# Calculate attention weights for the input sequence\n","attention_context = model.predict([padded_input, decoder_hidden_state])\n","\n","# Print attention weights and corresponding words\n","for i, attention_weight in enumerate(attention_weights):\n","    print(\"Attention weight for sentence\", i+1)\n","    for j, word in enumerate(input_seq[i]):\n","        print(f\"Word: {word}, Attention weight: {attention_weight[j]}\")\n","    print()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":609},"executionInfo":{"elapsed":6258,"status":"error","timestamp":1685825113769,"user":{"displayName":"Loretta","userId":"14548570598164544149"},"user_tz":420},"id":"gEiDQunq9qxn"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-6-9beebfac5d65\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 76\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Calculate attention weights for the input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 76\u001b[0;31m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-6-9beebfac5d65\u003e\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, encoder_hidden_states, decoder_hidden_state)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         alignment_scores = self.v_a(\n\u001b[0;32m---\u003e 20\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         )\n\u001b[1;32m     22\u001b[0m         \u001b[0mattention_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malignment_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'alignment_model_5' (type AlignmentModel).\n\nInput 0 of layer \"dense_15\" is incompatible with the layer: expected axis -1 of input shape to have value 32, but received input with shape (3, 9)\n\nCall arguments received by layer 'alignment_model_5' (type AlignmentModel):\n  • encoder_hidden_states=tf.Tensor(shape=(3, 9), dtype=int32)\n  • decoder_hidden_state=array([ 0.089265  ,  0.33094949,  0.81678946, -0.31488472,  1.75430771,\n        0.28022725,  0.84463163,  0.17384117, -0.20818981,  0.03185925,\n       -0.48045543,  1.58770113,  0.43934876,  0.63009106,  0.71074275,\n        1.88591307,  0.80945068, -1.50480396, -0.16374427,  0.57552244,\n       -1.84021568,  0.898263  , -1.62639512, -0.81326663,  0.33851587,\n       -0.99371013, -0.57192349, -1.3824729 , -1.87729899,  0.89791571,\n       -0.00438716,  0.89420579])"]}],"source":["\"\"\"\n","When generating the random decoder hidden state, I used np.random.randn\n","(1, hidden_size) to ensure it has the shape (1, hidden_size).\n","When predicting with the model, I passed [padded_input, decoder_hidden_state] as inputs to model.predict() to ensure the dimensions match correctly.\n","\"\"\"\n","!pip install --upgrade pip\n","import tensorflow as tf\n","import numpy as np\n","\n","# Define the alignment model\n","class AlignmentModel(tf.keras.Model):\n","    def __init__(self, hidden_size):\n","        super(AlignmentModel, self).__init__()\n","        self.W_a = tf.keras.layers.Dense(hidden_size)\n","        self.U_a = tf.keras.layers.Dense(hidden_size)\n","        self.v_a = tf.keras.layers.Dense(1)\n","\n","    def call(self, encoder_hidden_states, decoder_hidden_state):\n","        alignment_scores = self.v_a(\n","            tf.nn.tanh(self.W_a(encoder_hidden_states) + self.U_a(decoder_hidden_state))\n","        )\n","        attention_weights = tf.nn.softmax(alignment_scores, axis=1)\n","        return attention_weights\n","\n","# Create input sequences\n","input_seq = [\n","    ['word1', 'word2', 'word3', 'word4', 'word5'],\n","    ['word6', 'word7', 'word8', 'word9', 'word10', 'word11'],\n","    ['word12', 'word13', 'word14', 'word15', 'word16', 'word17', 'word18', 'word19', 'word20']\n","]\n","\n","# Convert words to indices\n","word_to_index = {}\n","index_to_word = {}\n","for sentence in input_seq:\n","    for word in sentence:\n","        if word not in word_to_index:\n","            word_to_index[word] = len(word_to_index)\n","            index_to_word[len(index_to_word)] = word\n","\n","# Convert input sequences to indices\n","input_indices = []\n","for sentence in input_seq:\n","    sentence_indices = [word_to_index[word] for word in sentence]\n","    input_indices.append(sentence_indices)\n","\n","# Pad sequences to have equal length\n","max_length = max(len(sentence) for sentence in input_indices)\n","padded_input = tf.keras.preprocessing.sequence.pad_sequences(\n","    input_indices, maxlen=max_length, padding='post'\n",")\n","\n","# Build and compile the model\n","hidden_size = 32\n","encoder_inputs = tf.keras.layers.Input(shape=(max_length,))\n","decoder_hidden_state = tf.keras.layers.Input(shape=(hidden_size,))\n","\n","\n","\n","\n","\n","encoder_hidden_states = tf.keras.layers.Embedding(len(word_to_index), hidden_size)(encoder_inputs)\n","attention_model = AlignmentModel(hidden_size)\n","attention_weights = attention_model(encoder_hidden_states, decoder_hidden_state)\n","attention_context = tf.reduce_sum(attention_weights * encoder_hidden_states, axis=1)\n","\n","model = tf.keras.Model(inputs=[encoder_inputs, decoder_hidden_state], outputs=attention_context)\n","model.compile(optimizer='adam', loss='mse')\n","\n","# Generate random decoder hidden state\n","decoder_hidden_state = np.random.randn(hidden_size)\n","\n","\n","\n","# Calculate attention weights for the input sequence\n","attention_weights = attention_model(padded_input, decoder_hidden_state)\n","attention_weights = tf.squeeze(attention_weights, axis=-1).numpy()\n","\n","# Print attention weights and corresponding words\n","for i, sentence in enumerate(input_seq):\n","    print(\"Attention weights for sentence\", i+1)\n","    for j, word in enumerate(sentence):\n","        print(f\"Word: {word}, Attention weight: {attention_weights[i][j]}\")\n","    print()\n","\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNmXRXCzj2dcS6o20XCG9AE","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}