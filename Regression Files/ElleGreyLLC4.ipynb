{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"A3x-GOn8KpGd"},"outputs":[],"source":["#RUN THS SNIPPET FIRST DOWNLOAD MODEL FOR FACE LANDMARKS FROM GITHUB\n","!pip install dlib opencv-python-headless imutils matplotl\n","\n","import dlib\n","import urllib.request\n","import bz2\n","import os\n","\n","# Define the URL of the compressed model file on GitHub\n","model_url = \"https://github.com/davisking/dlib-models/raw/master/shape_predictor_68_face_landmarks.dat.bz2\"\n","\n","# Define the local file path where you want to save the model\n","model_path = \"shape_predictor_68_face_landmarks.dat\"\n","\n","# Check if the model file already exists, if not, download and extract it\n","if not os.path.exists(model_path):\n","    print(\"Downloading and extracting the model...\")\n","    urllib.request.urlretrieve(model_url, model_path + \".bz2\")\n","    with bz2.BZ2File(model_path + \".bz2\", \"rb\") as source, open(model_path, \"wb\") as dest:\n","        dest.write(source.read())\n","    print(\"Model downloaded and extracted successfully!\")\n","\n","# Load the pre-trained shape predictor model for facial landmarks\n","predictor = dlib.shape_predictor(model_path)\n","\n","\n","#RUN THIS SNIPPET NEXT TO GET DLIB AND OTHER LIBRARIES\n","# Install dlib without uninstalling it and without CUDA support\n","#!pip install dlib --no-cache-dir --force-reinstall\n","import os\n","\n","# Force dlib to use CPU (hide GPUs)\n","os.environ['CUDA_VISIBLE_DEVICES'] = ''\n","\n","# Import other necessary libraries\n","import dlib\n","# Import other libraries and modules you need for your project\n","\n","import urllib.request\n","import bz2\n","import os\n","\n","# Replace the model_path variable with the specific path to the model file\n","model_path = \"/content/dlib_face_recognition_resnet_model_v1.dat\"\n","\n","# Check if the model file already exists, if not, download and extract it\n","if not os.path.exists(model_path):\n","    print(\"Downloading and extracting the model...\")\n","    urllib.request.urlretrieve(model_url, model_path + \".bz2\")\n","    with bz2.BZ2File(model_path + \".bz2\", \"rb\") as source, open(model_path, \"wb\") as dest:\n","        dest.write(source.read())\n","    print(\"Model downloaded and extracted successfully!\")\n","\n","# Load the pre-trained face recognition model\n","facerec = '/content/dlib_face_recognition_resnet_model_v1.dat'\n","\n","\n","# The rest of your code here\n","\n","#RUN THIS PREDICTOR MODEL FOR FACIAL ANDMARKS\n","\n","!pip install --upgrade pip\n","# First, install the necessary libraries\n","!pip install dlib opencv-python-headless imutils moviepy\n","!pip install opencv-python-headless\n","!pip install moviepy\n","!pip install ffmpeg-python\n","\n","import cv2\n","import dlib\n","import numpy as np\n","import moviepy.editor as mp\n","import time\n","\n","from IPython.display import Audio, display, Image\n","from moviepy.editor import AudioFileClip\n","from moviepy.editor import VideoFileClip\n","from moviepy.editor import ImageClip\n","from moviepy.editor import CompositeVideoClip\n","from moviepy.editor import VideoFileClip, CompositeVideoClip, clips_array\n","from moviepy.editor import AudioFileClip\n","\n","# Load pre-trained shape predictor model for facial landmarks\n","shape_predictor_path = 'shape_predictor_68_face_landmarks.dat'\n","predictor = dlib.shape_predictor(shape_predictor_path)\n","\n","# Load pre-trained deep learning model for face detection\n","face_rec_model_path = 'dlib_face_recognition_resnet_model_v1.dat'\n","detector = dlib.get_frontal_face_detector()\n","\n","# Load the song\n","song_url = \"/content/sample_data/when_you_believe_audio.mp3\"\n","song_clip = mp.AudioFileClip(song_url)\n","song_duration = 110  # 1:50 (110 seconds)\n","\n","# Load the input JPEG images\n","input_image_path1 = '/content/sample_data/IMG-Singer1.jpg'\n","image1 = cv2.imread(input_image_path1)\n","gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n","\n","# Load the input JPEG images\n","input_image_path2 = '/content/sample_data/IMG-Singer2.jpg'\n","image2 = cv2.imread(input_image_path2)\n","gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n","\n","# Detect faces in the input images\n","faces1 = detector(gray1)\n","faces2 = detector(gray2)\n","\n","# Assuming there's only one face detected in each image\n","if len(faces1) > 0 and len(faces2) > 0:\n","    face1 = faces1[0]\n","    face2 = faces2[0]\n","\n","    # Get facial landmarks for the first image\n","    landmarks1 = predictor(gray1, face1)\n","    mouth_coords1 = [(landmarks1.part(i).x, landmarks1.part(i).y) for i in range(48, 68)]\n","\n","    # Get facial landmarks for the second image\n","    landmarks2 = predictor(gray2, face2)\n","    mouth_coords2 = [(landmarks2.part(i).x, landmarks2.part(i).y) for i in range(48, 68)]\n","\n","    # Define mouth region coordinates for the first image\n","    x1, y1, w1, h1 = cv2.boundingRect(np.array(mouth_coords1))\n","\n","    # Define mouth region coordinates for the second image\n","    x2, y2, w2, h2 = cv2.boundingRect(np.array(mouth_coords2))\n","\n","    # Draw a green square around the detected face in the first image\n","    cv2.rectangle(image1, (face1.left(), face1.top()), (face1.right(), face1.bottom()), (0, 255, 0), 2)\n","\n","    # Draw a red square around the mouth in the first image\n","    cv2.rectangle(image1, (x1, y1), (x1 + w1, y1 + h1), (0, 0, 255), 2)\n","\n","    # Draw a green square around the detected face in the second image\n","    cv2.rectangle(image2, (face2.left(), face2.top()), (face2.right(), face2.bottom()), (0, 255, 0), 2)\n","\n","    # Draw a red square around the mouth in the second image\n","    cv2.rectangle(image2, (x2, y2), (x2 + w2, y2 + h2), (0, 0, 255), 2)\n","\n","    # Display the first image with squares and add an audio button\n","    image_with_squares_path1 = '/content/sample_data/image_with_squares1.jpeg'\n","    cv2.imwrite(image_with_squares_path1, image1)\n","    display(Image(filename=image_with_squares_path1, width=400))\n","    display(Audio(filename='/content/sample_data/when_you_believe_audio.mp3'))\n","\n","    # Display the second image with squares and add an audio button\n","    image_with_squares_path2 = '/content/sample_data/image_with_squares2.jpeg'\n","    cv2.imwrite(image_with_squares_path2, image2)\n","    display(Image(filename=image_with_squares_path2, width=400))\n","    display(Audio(filename='/content/sample_data/when_you_believe_audio.mp3'))\n","else:\n","    print(\"No face detected in one or both input images after trying multiple rotations.\")\n","\n","\n","# Use OpenCV to detect and track mouth movements\n","#def detect_mouth_movement(image):\n","    # You can use OpenCV to detect the mouth movements here\n","    # This is a placeholder for your code\n","\n","    # Example: detect mouth region and create a mask\n","    # mask = cv2.inRange(image, lower_mouth_color, upper_mouth_color)\n","\n","    # You can track the movement over time\n","\n","    #return mouth_movement\n","\n","# Load the .jpg images for overlay\n","image1 = mp.ImageClip(\"/content/sample_data/IMG-Singer1.jpg\")\n","image2 = mp.ImageClip(\"/content/sample_data/IMG-Singer2.jpg\")\n","image3 = mp.ImageClip(\"/content/sample_data/image_with_squares1.jpeg\")\n","image4 = mp.ImageClip(\"/content/sample_data/image_with_squares2.jpeg\")\n","\n","\n","#Load the mouth movement videos\n","video1 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","video2 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","video3 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","video4 = VideoFileClip(\"/content/sample_data/IMG_1947.3gp\")\n","\n","\n","# Set the duration for overlay (1:50 - 0:33 seconds)\n","overlay_duration1 = song_duration - 33\n","\n","# Set the duration for overlay (2:27 - 0:33 seconds)\n","overlay_duration2 = song_duration - 33\n","\n","# Set the duration for overlay (1:50 - 0:33 seconds)\n","overlay_duration3 = song_duration - 33\n","\n","# Set the duration for overlay (2:27 - 0:33 seconds)\n","overlay_duration4 = song_duration - 33\n","\n","# Overlay the words over the images\n","overlay1 = mp.CompositeVideoClip([image1.set_duration(overlay_duration1)])\n","overlay2 = mp.CompositeVideoClip([image2.set_duration(overlay_duration2)])\n","overlay3 = mp.CompositeVideoClip([image3.set_duration(overlay_duration3)])\n","overlay4 = mp.CompositeVideoClip([image4.set_duration(overlay_duration4)])\n","\n","\n","# Create the script audio\n","script_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","\n","# Combine images and script audio\n","final_clip1 = mp.clips_array([[overlay1]])\n","final_clip1 = final_clip1.set_audio(script_audio)\n","\n","# Combine images and script audio\n","final_clip2 = mp.clips_array([[overlay2]])\n","final_clip2 = final_clip2.set_audio(script_audio)\n","\n","\n","# Combine images and script audio\n","final_clip3 = mp.clips_array([[overlay3]])\n","final_clip3 = final_clip3.set_audio(script_audio)\n","\n","\n","# Combine images and script audio\n","final_clip4 = mp.clips_array([[overlay4]])\n","final_clip4 = final_clip4.set_audio(script_audio)\n","\n","\n","# Add an audio button to play the song\n","\n","song_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","final_clip1 = final_clip1.set_audio(song_audio)\n","\n","song_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","final_clip2 = final_clip2.set_audio(song_audio)\n","\n","\n","ong_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","final_clip3 = final_clip3.set_audio(song_audio)\n","\n","song_audio = mp.AudioFileClip(\"/content/sample_data/when_you_believe_audio.mp3\")\n","final_clip4 = final_clip4.set_audio(song_audio)\n","\n","# Set the duration for the images to match the videos\n","image1 = image1.set_duration(video1.duration)\n","image2 = image2.set_duration(video2.duration)\n","image3 = image3.set_duration(video3.duration)\n","image4 = image4.set_duration(video4.duration)\n","\n","# Create a video clip with the images side by side\n","side_by_side1 = clips_array([[image1, image2]])\n","side_by_side2 = clips_array([[image3, image4]])\n","\n","\n","# Overlay the mouth movement videos on the images\n","video1 = video1.set_position(\"center\")\n","video2 = video2.set_position(\"center\")\n","video3 = video3.set_position(\"center\")\n","video4 = video4.set_position(\"center\")\n","\n","final_clip1 = CompositeVideoClip([side_by_side1, video1, video2])\n","final_clip2 = CompositeVideoClip([side_by_side2, video3, video4])\n","\n","# Overlay the mouth movement video on top of a black background\n","mouth_movement_overlay1 = video1.set_position(\"center\").on_color(\n","    (1920, 1080), color=(0, 0, 0), pos=\"center\")\n","\n","# Overlay the mouth movement video on top of a black background\n","mouth_movement_overlay2 = video2.set_position(\"center\").on_color(\n","    (1920, 1080), color=(0, 0, 0), pos=\"center\")\n","\n","# Overlay the mouth movement video on top of a black background\n","mouth_movement_overlay3 = video3.set_position(\"center\").on_color(\n","    (1920, 1080), color=(0, 0, 0), pos=\"center\")\n","\n","# Overlay the mouth movement video on top of a black background\n","mouth_movement_overlay4 = video4.set_position(\"center\").on_color(\n","    (1920, 1080), color=(0, 0, 0), pos=\"center\")\n","\n","\n","# Combine the mouth movement overlay with the audio\n","final_clip1 = CompositeVideoClip([mouth_movement_overlay1.set_audio(song_audio)])\n","\n","#Combine the mouth movement overlay with the audio\n","final_clip2 = CompositeVideoClip([mouth_movement_overlay2.set_audio(song_audio)])\n","\n","# Combine the mouth movement overlay with the audio\n","final_clip3 = CompositeVideoClip([mouth_movement_overlay3.set_audio(song_audio)])\n","\n","#Combine the mouth movement overlay with the audio\n","final_clip4 = CompositeVideoClip([mouth_movement_overlay4.set_audio(song_audio)])\n","\n","\"\"\"\n","# Set the duration for overlay (1:50 - 0:33 seconds)\n","overlay_duration1 = song_duration - 33\n","\n","\n","# Set the duration for overlay (2:47 - 0:33 seconds)\n","overlay_duration2 = song_duration - 150\n","\n","# Set the duration for overlay (1:50 - 0:33 seconds)\n","overlay_duration3 = song_duration - 33\n","\n","# Set the duration for overlay (2:47 - 0:33 seconds)\n","overlay_duration4 = song_duration - 150\n","\n","# Overlay the words over the images\n","overlay1 = mp.CompositeVideoClip([image1.set_duration(overlay_duration1)])\n","\n","\n","# Set the audio of the final video to be the same as one of the videos (assuming audio is the same for both)\n","final_clip1 = final_clip1.set_audio(video1.audio)\n","\"\"\"\n","\n","# Write the final video1\n","final_clip1.write_videofile(\"/content/sample_data/final1_output.mp3\", codec=\"libx264\", audio_codec=\"aac\")\n","\n","#final_clip1.write_audiofile(\"/content/sample_data/final1_output.mp3\")\n","\n","# Extract the audio from final_clip1\n","final_audio1 = final_clip1.audio\n","\n","# Write the extracted audio to an MP3 file\n","final_audio1.write_audiofile(\"/content/sample_data/final1_output.mp3\")\n","\n","# Display the audio playback button for the song\n","final_clip1.ipython_display(t=0, width=800)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEMZvfZZKrus"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ILzhoRljAt2F","colab":{"base_uri":"https://localhost:8080/","height":721},"executionInfo":{"status":"error","timestamp":1698125333543,"user_tz":420,"elapsed":24,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"832b4084-e0f7-4a84-a643-24c90f262f63"},"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m             \u001b[0msubprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/tools.py\u001b[0m in \u001b[0;36msubprocess_call\u001b[0;34m(cmd, logger, errorprint)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    972\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1862\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'unset'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-218728aeb830>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create a scrolling text clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtext_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"white\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbg_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"black\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Set the duration of the text clip to match the audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/VideoClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, txt, filename, size, color, bg_color, fontsize, font, stroke_color, stroke_width, method, kerning, align, interline, tempfilename, temptxt, transparent, remove_temp, print_cmd)\u001b[0m\n\u001b[1;32m   1144\u001b[0m                         \u001b[0;34m\"ImageMagick binary in file conf.py, or that the path \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m                         \"you specified is incorrect\"))\n\u001b[0;32m-> 1146\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0mImageClip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: MoviePy Error: creation of None failed because of the following error:\n\n[Errno 2] No such file or directory: 'unset'.\n\n.This error can be due to the fact that ImageMagick is not installed on your computer, or (for Windows users) that you didn't specify the path to the ImageMagick binary in file conf.py, or that the path you specified is incorrect"]}],"source":["#------- Extra Code for Ovrdrlay mp3 Fish text\n","\n","import cv2\n","import dlib\n","import numpy as np\n","import moviepy.editor as mp\n","\n","# Load the image\n","image_with_squares1 = mp.ImageClip(\"/content/sample_data/image_with_squares1.jpg\")\n","\n","# Load the audio\n","audio = mp.AudioFileClip(\"/content/sample_data/FiahOnABicycle.m4a\")\n","\n","# Load the text from the file\n","with open(\"/content/sample_data/fish_words.txt\", \"r\") as file:\n","    text = file.read()\n","\n","# Create a scrolling text clip\n","text_clip = mp.TextClip(text, fontsize=20, color=\"white\", bg_color=\"black\")\n","\n","# Set the duration of the text clip to match the audio\n","text_clip = text_clip.set_duration(audio.duration)\n","\n","# Overlay the text on the image\n","video = mp.CompositeVideoClip([image_with_squares1, text_clip])\n","\n","# Set the audio of the video to match the audio file\n","video = video.set_audio(audio)\n","\n","# Write the final video with text overlay\n","video.write_videofile(\"final_video_with_text.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1FnIEVtkNPqHY-llCaK5mVHCeMt1n5pzc","timestamp":1698116387849},{"file_id":"1M9V8vzFPLMPIy3FEmxSjevfm-i17KqCx","timestamp":1697435986751}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}