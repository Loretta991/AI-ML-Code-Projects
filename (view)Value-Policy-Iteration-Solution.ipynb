{"cells":[{"cell_type":"markdown","metadata":{"id":"x65DWDE-ay7h"},"source":["z# Week 12 - Sequential Decision Making I\n","## Value and Policy Iteration Solutions"]},{"cell_type":"markdown","metadata":{"id":"Ifsk8KF-ay7o"},"source":["Author: Massimo Caccia massimo.p.caccia@gmail.com <br>\n","\n","The code was Adapted from: https://github.com/lazyprogrammer/machine_learning_examples/tree/master/rl <br>\n","and then from: https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo"]},{"cell_type":"markdown","metadata":{"id":"bqGFcrqcay7p"},"source":["## 0. Preliminaries\n","\n","Before we jump into the value and policy iteration excercies, we will test your comprehension of a Markov Decision Process (MDP). <br>"]},{"cell_type":"markdown","metadata":{"id":"K8wQeH0Eay7q"},"source":["### 0.1 Tic-Tac-Toe\n","\n","Let's take a simple example: Tic-Tac-Toe (also known as Tic-tac-toe, noughts and crosses, or Xs and Os). Definition: it is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row is the winner."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eepxUNfRay7s","outputId":"be45e952-d97e-4815-b323-c081cd279657"},"outputs":[{"data":{"text/html":["<img src=\"https://bjc.edc.org/bjc-r/img/3-lists/TTT1_img/Three%20States%20of%20TTT.png\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["from IPython.display import Image\n","from IPython.core.display import HTML \n","Image(url= \"https://bjc.edc.org/bjc-r/img/3-lists/TTT1_img/Three%20States%20of%20TTT.png\")"]},{"cell_type":"markdown","metadata":{"id":"CGySLJWUay7x"},"source":["**Question:** Imagine you were trying to build an agent for this game. Let's try to describe how we would model it. Specifically, what are the states, actions, transition function and rewards?"]},{"cell_type":"markdown","metadata":{"id":"idV_rlpZay7y"},"source":["**Answer:**<br>\n","The **state space** is a 3x3 Matrix or a vector of length 9 that indicates if a particular spot is: a) empty, b) taken by X or c) taken by O. <br>\n","\n","The **actions** are on which of the 9 spot you can play (so there is 9 possible actions). Note that as the game evolves, some actions will become unavailable. <br>\n","\n","An example of a **reward function** could return +1 if you win, -1 if you lose, and 0 for a draw.\n","\n","The **transition function** is dictated by your opponent's strategy. <br>"]},{"cell_type":"markdown","metadata":{"id":"U0uOotf7ay70"},"source":["### 0.2 Recommender Systems\n","\n","**Question:** In the last class we discussed recommender systems. Imagine that you would like to model the recommendation process overe time as an MDP. How would you do it?"]},{"cell_type":"markdown","metadata":{"id":"l2zeaDz7ay71"},"source":["**Answer:**\n","\n","**States:** You would like the state to encode the user's preferences. There are different ways of doing so. Here is one: the state lists all the items previously consumed by the user.\n","\n","**Actions:** which item to recommend (item 1, item 2, ... item n). Number of actions is the number of items.\n","\n","**Reward:** +1 if the user consumes the recommeded item, -1 if not.\n","\n","**Transition Probabilities:** that will depend on the user."]},{"cell_type":"markdown","metadata":{"id":"t7K1O-PCay72"},"source":["## 1. Value Iteration"]},{"cell_type":"markdown","metadata":{"id":"Ma6VBg28ay73"},"source":["The exercises will test your capacity to **complete the value iteration algorithm**.\n","\n","You can find details about the algorithm at slide 46 of the [slide](http://www.cs.toronto.edu/~lcharlin/courses/80-629/slides_rl.pdf) deck. <br>\n","\n","The algorithm will be tested on a simple Gridworld similar to the one presented at slide 12."]},{"cell_type":"markdown","metadata":{"id":"IJPt2IhAay74"},"source":["### 1.1 Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oKgnaE4Aay75","outputId":"2863d5c4-e387-4833-bead-5b07c6a59e97"},"outputs":[{"name":"stdout","output_type":"stream","text":["File ‘gridWorldGame.py’ already there; not retrieving.\r\n","\r\n"]}],"source":["#imports\n","\n","!wget -nc https://raw.githubusercontent.com/lcharlin/80-629/master/week12-MDPs/gridWorldGame.py\n","    \n","import numpy as np\n","from gridWorldGame import standard_grid, negative_grid, print_values, print_policy"]},{"cell_type":"markdown","metadata":{"id":"PJVk9ho7ay77"},"source":["Let's set some variables. <br>\n","`SMALL_ENOUGH` is a threshold we will utilize to determine the convergence of value iteration<br>\n","`GAMMA` is the discount factor denoted $\\gamma$ in the slides (see slide 36) <br>\n","`ALL_POSSIBLE_ACTIONS` are the actions you can take in the GridWold, as in slide 12. In this simple grid world, we will have four actions: Up, Down, Right, Left. <br>\n","`NOISE_PROB` defines how stochastic the environement is. It is the probability that the environment takes you where a random action would. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKnaC6cOay79"},"outputs":[],"source":["SMALL_ENOUGH = 1e-3 # threshold to declare convergence\n","GAMMA = 0.9         # discount factor\n","ALL_POSSIBLE_ACTIONS = ('U', 'D', 'L', 'R') # Up, Down, Left, Right\n","NOISE_PROB = 0.1    # Probability of the agent not reaching it's intended goal after an action"]},{"cell_type":"markdown","metadata":{"id":"F7E9yJRlay7-"},"source":["Now we will set up a the Gridworld. <br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wb2YR92Iay7-","outputId":"62b4f228-710a-4254-9b50-4af0fe959b89"},"outputs":[{"name":"stdout","output_type":"stream","text":["rewards:\n","---------------------------\n"," 0.00| 0.00| 0.00| 1.10|\n","---------------------------\n"," 0.00| 0.00| 0.00|-1.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n"]}],"source":["grid = standard_grid(noise_prob=NOISE_PROB)\n","print(\"rewards:\")\n","print_values(grid.rewards, grid)"]},{"cell_type":"markdown","metadata":{"id":"oxrAVXPJay8A"},"source":["There are three absorbing states: (0,3),(1,3), and (1,1)"]},{"cell_type":"markdown","metadata":{"id":"8yAmT8msay8A"},"source":["Next, we will define a random inital policy $\\pi$. <br>\n","Remember that a policy maps states to actions $\\pi : S \\rightarrow A$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nqu9wOzaay8A","outputId":"4e6cc524-a9d4-4468-a9dc-e89a5482fd1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["initial policy:\n","---------------------------\n","  D  |  L  |  U  | N/A |\n","---------------------------\n","  R  | N/A |  R  | N/A |\n","---------------------------\n","  U  |  D  |  D  |  U  |\n"]}],"source":["policy = {}\n","for s in grid.actions.keys():\n","    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n","\n","# initial policy\n","print(\"initial policy:\")\n","print_policy(policy, grid)"]},{"cell_type":"markdown","metadata":{"id":"wv9Q_cm_ay8G"},"source":["Note that there is no policy in the absorbing/terminal states (hence the Not Available \"N/A\")"]},{"cell_type":"markdown","metadata":{"id":"QtwfJJCEay8H"},"source":["Next, we will randomly initialize the value fonction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1iERavNiay8H","outputId":"164a08f3-c120-4c9e-c304-e413f86dbb30"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------\n"," 0.44| 0.19| 0.96| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.62| 0.00|\n","---------------------------\n"," 0.78| 0.79| 0.28| 0.27|\n"]}],"source":["np.random.seed(1234) # make sure this is reproducable\n","\n","V = {}\n","states = grid.all_states()\n","for s in states:\n","    # V[s] = 0\n","    if s in grid.actions:\n","        V[s] = np.random.random()\n","    else:\n","        # terminal state\n","        V[s] = 0\n","\n","# initial value for all states in grid\n","print_values(V, grid)"]},{"cell_type":"markdown","metadata":{"id":"XyFyZW0may8I"},"source":["Note that we set to Null the values of the terminal states. <br> \n","For the print_values() function to compile, we set them to 0."]},{"cell_type":"markdown","metadata":{"id":"HhVsBw0hay8J"},"source":["### 1.2 Value iteration algorithms - code completion\n","\n","You will now have to complete the Value iteration algorithm. <br>\n","Remember that, for each iteration, each state s need to have to be update with the formula:\n","\n","$$\n","V(s) = \\underset{a}{max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V(s') \\big\\}\n","$$\n","Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n","Also, remember that in value iteration, the policy is implicit. <br> Thus, you don't need to update it at every iteration. <br>\n","Run the algorithm until convergence."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"EWmnG8rxay8K","outputId":"17716595-225a-4f65-fe3a-1c71341124f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["VI iteration 0: \n","---------------------------\n"," 0.44| 0.19| 0.96| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.62| 0.00|\n","---------------------------\n"," 0.78| 0.79| 0.28| 0.27|\n","\n","\n","\n","\t biggest change is: 0.670806 \n","\n","\n","VI iteration 1: \n","---------------------------\n"," 0.78| 0.86| 1.10| 0.00|\n","---------------------------\n"," 0.72| 0.00| 0.86| 0.00|\n","---------------------------\n"," 0.72| 0.71| 0.78| 0.25|\n","\n","\n","\n","\t biggest change is: 0.449666 \n","\n","\n","VI iteration 2: \n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.70| 0.00|\n","---------------------------\n"," 0.65| 0.70| 0.63| 0.70|\n","\n","\n","\n","\t biggest change is: 0.291516 \n","\n","\n","VI iteration 3: \n","---------------------------\n"," 0.89| 0.99| 0.99| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.63| 0.89| 0.63|\n","\n","\n","\n","\t biggest change is: 0.173265 \n","\n","\n","VI iteration 4: \n","---------------------------\n"," 0.80| 0.89| 1.10| 0.00|\n","---------------------------\n"," 0.72| 0.00| 0.89| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.80| 0.80|\n","\n","\n","\n","\t biggest change is: 0.099000 \n","\n","\n","VI iteration 5: \n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.65| 0.72| 0.89| 0.72|\n","\n","\n","\n","\t biggest change is: 0.080190 \n","\n","\n","VI iteration 6: \n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","\n","\n","\n","\t biggest change is: 0.000000 \n","\n","\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n"]}],"source":["iteration=0\n","while True:\n","    print(\"VI iteration %d: \" % iteration)\n","    print_values(V, grid)\n","    print(\"\\n\\n\")\n","  \n","    biggest_change = 0\n","    for s in states:\n","        old_v = V[s]\n","\n","        # V(s) only has value if it's not a terminal state\n","        if s in policy:\n","            new_v = float('-inf')\n","\n","            # for each action\n","            for a in ALL_POSSIBLE_ACTIONS:\n","                grid.set_state(s)\n","                r = grid.move(a)\n","                sprime = grid.current_state()\n","                #  - compute this V[s] = max[a]{ sum[s',r] { p(s',r|s,a)[r + gamma*V[s']] } }\n","                v = r + GAMMA * V[sprime]\n","                if v > new_v: # is this the best action so far\n","                    new_v = v\n","            V[s] = new_v\n","            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n","\n","    print('\\t biggest change is: %f \\n\\n' % biggest_change)\n","    if biggest_change < SMALL_ENOUGH:\n","        break\n","    iteration+=1\n","print_values(V, grid)"]},{"cell_type":"markdown","metadata":{"id":"4q9cZKjKay8L"},"source":["Now that the value function is optimized, use it to find the optimal policy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qk3-eMfKay8L"},"outputs":[],"source":["deterministic_grid = standard_grid(noise_prob=0.)\n","\n","for s in policy.keys():\n","    best_a = None\n","    best_value = float('-inf')\n","    # loop through all possible actions to find the best current action\n","    for a in ALL_POSSIBLE_ACTIONS:\n","        deterministic_grid.set_state(s)\n","        r = deterministic_grid.move(a)\n","        v = r + GAMMA * V[deterministic_grid.current_state()]\n","        if v > best_value:\n","            best_value = v\n","            best_a = a\n","    policy[s] = best_a"]},{"cell_type":"markdown","metadata":{"id":"9FOLwN6Pay8M"},"source":["Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2TjImX9ay8M","outputId":"9b3002fb-48e0-4fa4-c937-000125b7f9d5"},"outputs":[{"name":"stdout","output_type":"stream","text":["values:\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","\n","policy:\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n"]}],"source":["print(\"values:\")\n","print_values(V, grid)\n","print(\"\\npolicy:\")\n","print_policy(policy, grid)"]},{"cell_type":"markdown","metadata":{"id":"AEXrhOp2ay8N"},"source":["## 2. Policy Iteration"]},{"cell_type":"markdown","metadata":{"id":"Tv8M2uExay8N"},"source":["You will be tested on your capacity to **complete the poliy iteration algorithm**. <br>\n","You can find details about the algorithm at slide 47 of the slide deck. <br>\n","The algorithm will be tested on a simple Gridworld similar to the one presented at slide 12. <br>\n","This Gridworld is however simpler because the MDP is deterministic. <br>"]},{"cell_type":"markdown","metadata":{"id":"vgqya5Buay8N"},"source":["First we will define a random inital policy. <br>\n","Remember that a policy maps states to actions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngQ1_DNIay8N","outputId":"8142436a-0bcb-4773-e27a-2b050f000b40"},"outputs":[{"name":"stdout","output_type":"stream","text":["initial policy:\n","---------------------------\n","  R  |  U  |  L  | N/A |\n","---------------------------\n","  R  | N/A |  D  | N/A |\n","---------------------------\n","  U  |  D  |  D  |  L  |\n"]}],"source":["policy = {}\n","for s in grid.actions.keys():\n","    policy[s] = np.random.choice(ALL_POSSIBLE_ACTIONS)\n","\n","# initial policy\n","print(\"initial policy:\")\n","print_policy(policy, grid)"]},{"cell_type":"markdown","metadata":{"id":"6O-TUGCYay8O"},"source":["Next, we will randomly initialize the value fonction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3H0PSKpgay8O","outputId":"c5861367-5e51-4519-ec22-102eb95c814a"},"outputs":[{"name":"stdout","output_type":"stream","text":["---------------------------\n"," 0.62| 0.95| 0.09| 0.00|\n","---------------------------\n"," 0.59| 0.00| 0.10| 0.00|\n","---------------------------\n"," 0.01| 0.83| 0.58| 0.32|\n"]}],"source":["np.random.seed(1234)\n","\n","# initialize V(s) - value function\n","V = {}\n","states = grid.all_states()\n","for s in states:\n","    if s in grid.actions:\n","        V[s] = np.random.random()\n","    else:\n","        # terminal state\n","        V[s] = 0\n","\n","# initial value for all states in grid\n","print_values(V, grid)"]},{"cell_type":"markdown","metadata":{"id":"6fz_1psOay8O"},"source":["Note that we set to Null the values of the terminal states. <br> \n","For the print_values() function to compile, we set them to 0."]},{"cell_type":"markdown","metadata":{"id":"ar_XnGmUay8P"},"source":["### 2.2 Policy iteration - code completion\n","\n","You will now have to complete the Policy iteration algorithm. <br>\n","Remember that the algorithm works in two phases. <br>\n","First, in the *policy evaluation* phase, the value function is update with the formula:\n","\n","$$\n","V^\\pi(s) =  \\sum_{s'}  p(s'|s,\\pi(s))(r + \\gamma*V^\\pi(s') \n","$$\n","This part of the algorithm is already coded for you. <br>\n","\n","Second, in the *policy improvement* step, the policy is updated with the formula:\n","\n","$$\n","\\pi'(s) = \\underset{a}{arg max}\\big\\{ \\sum_{s'}  p(s'|s,a)(r + \\gamma*V^\\pi(s') \\big\\}\n","$$\n","\n","This is the part of code you will have to complete. <br>\n","\n","Note that in the current gridWorld, p(s'|s,a) is deterministic. <br>\n","Run the algorithm until convergence."]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Rl7O8Gzray8P","outputId":"7027f581-daaf-42bf-8101-d191db9c999b"},"outputs":[{"name":"stdout","output_type":"stream","text":["values (iteration 0)\n","---------------------------\n"," 0.62| 0.95| 0.09| 0.00|\n","---------------------------\n"," 0.59| 0.00| 0.10| 0.00|\n","---------------------------\n"," 0.01| 0.83| 0.58| 0.32|\n","policy (iteration 0)\n","---------------------------\n","  R  |  D  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 1)\n","---------------------------\n"," 0.01| 0.01| 1.10| 0.00|\n","---------------------------\n"," 0.01| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.01| 0.80| 0.89| 0.80|\n","policy (iteration 1)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 2)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 2)\n","---------------------------\n","  R  |  R  |  U  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 3)\n","---------------------------\n"," 0.01| 0.01| 0.01| 0.00|\n","---------------------------\n"," 0.01| 0.00| 0.01| 0.00|\n","---------------------------\n"," 0.01| 0.01| 0.01| 0.01|\n","policy (iteration 3)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  R  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 4)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00|-0.01|\n","policy (iteration 4)\n","---------------------------\n","  R  |  R  |  L  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 5)\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.01| 0.00| 0.00|\n","policy (iteration 5)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  R  |  U  |  L  |  D  |\n","\n","\n","\n","values (iteration 6)\n","---------------------------\n"," 0.00| 0.00| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 6)\n","---------------------------\n","  L  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 7)\n","---------------------------\n"," 0.00| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.89| 0.00|\n","policy (iteration 7)\n","---------------------------\n","  L  |  R  |  U  | N/A |\n","---------------------------\n","  D  | N/A |  L  | N/A |\n","---------------------------\n","  D  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 8)\n","---------------------------\n","-0.01|-0.01|-0.01| 0.00|\n","---------------------------\n","-0.00| 0.00|-0.01| 0.00|\n","---------------------------\n","-0.00|-0.01|-0.01|-0.01|\n","policy (iteration 8)\n","---------------------------\n","  D  |  L  |  R  | N/A |\n","---------------------------\n","  L  | N/A |  D  | N/A |\n","---------------------------\n","  U  |  L  |  L  |  D  |\n","\n","\n","\n","values (iteration 9)\n","---------------------------\n","-0.00|-0.00| 1.10| 0.00|\n","---------------------------\n","-0.00| 0.00|-0.00| 0.00|\n","---------------------------\n","-0.00|-0.00|-0.00|-0.00|\n","policy (iteration 9)\n","---------------------------\n","  U  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  L  |  L  |\n","\n","\n","\n","values (iteration 10)\n","---------------------------\n","-0.00| 0.99| 1.10| 0.00|\n","---------------------------\n","-0.00| 0.00| 0.99| 0.00|\n","---------------------------\n","-0.00|-0.00|-0.00|-0.00|\n","policy (iteration 10)\n","---------------------------\n","  R  |  R  |  U  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  L  |  U  |  D  |\n","\n","\n","\n","values (iteration 11)\n","---------------------------\n"," 0.01| 0.01| 0.01| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.01| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.01| 0.01|\n","policy (iteration 11)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  D  |\n","\n","\n","\n","values (iteration 12)\n","---------------------------\n"," 0.00| 0.00| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.00|\n","policy (iteration 12)\n","---------------------------\n","  U  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 13)\n","---------------------------\n"," 0.58| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.65| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 13)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 14)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.65| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 14)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  D  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 15)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 15)\n","---------------------------\n","  R  |  R  |  D  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 16)\n","---------------------------\n","-0.00|-0.00|-0.00| 0.00|\n","---------------------------\n","-0.00| 0.00|-0.00| 0.00|\n","---------------------------\n","-0.00|-0.00|-0.00|-0.00|\n","policy (iteration 16)\n","---------------------------\n","  D  |  L  |  R  | N/A |\n","---------------------------\n","  L  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  L  |  D  |  L  |\n","\n","\n","\n","values (iteration 17)\n","---------------------------\n","-0.00|-0.00| 1.10| 0.00|\n","---------------------------\n","-0.00| 0.00| 0.99| 0.00|\n","---------------------------\n","-0.00|-0.00|-0.00|-0.00|\n","policy (iteration 17)\n","---------------------------\n","  U  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 18)\n","---------------------------\n","-0.00| 0.99| 1.10| 0.00|\n","---------------------------\n","-0.00| 0.00| 0.99| 0.00|\n","---------------------------\n","-0.00|-0.00| 0.89|-0.00|\n","policy (iteration 18)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 19)\n","---------------------------\n"," 0.01| 0.01| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.80| 0.89| 0.80|\n","policy (iteration 19)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 20)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 20)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 21)\n","---------------------------\n"," 0.00| 0.01| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.80| 0.89| 0.80|\n","policy (iteration 21)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 22)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 22)\n","---------------------------\n","  L  |  U  |  L  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 23)\n","---------------------------\n"," 0.00| 0.01| 0.01| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.01| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 23)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 24)\n","---------------------------\n"," 0.00| 0.00| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.89| 0.00|\n","policy (iteration 24)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  L  |\n","\n","\n","\n","values (iteration 25)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 25)\n","---------------------------\n","  L  |  D  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  L  |\n","\n","\n","\n","values (iteration 26)\n","---------------------------\n"," 0.00| 0.01| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.01| 0.89| 0.80|\n","policy (iteration 26)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  D  |  L  |\n","\n","\n","\n","values (iteration 27)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 27)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  D  |\n","\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["values (iteration 28)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.89| 0.00|\n","policy (iteration 28)\n","---------------------------\n","  R  |  R  |  D  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 29)\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 29)\n","---------------------------\n","  R  |  U  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  R  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 30)\n","---------------------------\n"," 0.00| 0.00| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 30)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 31)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.89| 0.00|\n","policy (iteration 31)\n","---------------------------\n","  R  |  D  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 32)\n","---------------------------\n"," 0.00| 0.00| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 32)\n","---------------------------\n","  U  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  D  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 33)\n","---------------------------\n"," 0.01| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.01| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.01| 0.80| 0.89| 0.80|\n","policy (iteration 33)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  D  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 34)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.65| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 34)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  R  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 35)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 35)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  U  |  U  |  U  |  L  |\n","\n","\n","\n","values (iteration 36)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00|-0.01| 0.00|\n","---------------------------\n"," 0.72|-0.01|-0.01|-0.01|\n","policy (iteration 36)\n","---------------------------\n","  D  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  L  |  D  |  D  |\n","\n","\n","\n","values (iteration 37)\n","---------------------------\n"," 0.00| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 37)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  D  |  U  |  U  |  D  |\n","\n","\n","\n","values (iteration 38)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.00| 0.00|\n","policy (iteration 38)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  L  | N/A |\n","---------------------------\n","  U  |  U  |  D  |  D  |\n","\n","\n","\n","values (iteration 39)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.00| 0.00|\n","---------------------------\n"," 0.72| 0.00| 0.00| 0.00|\n","policy (iteration 39)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  L  |  R  |  D  |\n","\n","\n","\n","values (iteration 40)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.65| 0.00| 0.00|\n","policy (iteration 40)\n","---------------------------\n","  D  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  L  |  U  |  D  |\n","\n","\n","\n","values (iteration 41)\n","---------------------------\n"," 0.00| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.00| 0.00| 0.89| 0.00|\n","policy (iteration 41)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  L  |  R  |  U  |  L  |\n","\n","\n","\n","values (iteration 42)\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","policy (iteration 42)\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n","\n","\n","\n"]}],"source":["iteration=0\n","# repeat until the policy does not change\n","while True:\n","    print(\"values (iteration %d)\" % iteration)\n","    print_values(V, grid)\n","    print(\"policy (iteration %d)\" % iteration)\n","    print_policy(policy, grid)\n","    print('\\n\\n')\n","\n","    # 1. policy evaluation step\n","    # this implementation does multiple policy-evaluation steps\n","    # this is different than in the algorithm from the slides \n","    # which does a single one.\n","    while True:\n","        biggest_change = 0\n","        for s in states:\n","            old_v = V[s]\n","\n","            # V(s) only has value if it's not a terminal state\n","            if s in policy:\n","                a = policy[s]\n","                grid.set_state(s)\n","                r = grid.move(a) # reward\n","                sprime = grid.current_state() # s' \n","                V[s] = r + GAMMA * V[sprime]\n","            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n","        if biggest_change < SMALL_ENOUGH:\n","            break\n","\n","    #2. policy improvement step\n","    is_policy_converged = True\n","    for s in states:\n","        if s in policy:\n","            old_a = policy[s]\n","            new_a = None\n","            best_value = float('-inf')\n","            # loop through all possible actions to find the best current action\n","            for a in ALL_POSSIBLE_ACTIONS:\n","                grid.set_state(s)\n","                r = grid.move(a)\n","                sprime = grid.current_state() \n","                v = r + GAMMA * V[sprime]\n","                if v > best_value:\n","                    best_value = v\n","                    new_a = a\n","            if new_a is None: \n","                print('problem')\n","            policy[s] = new_a\n","            if new_a != old_a:\n","                is_policy_converged = False\n","\n","    if is_policy_converged:\n","        break\n","    iteration+=1\n"]},{"cell_type":"markdown","metadata":{"id":"OCfC8Uy3ay8Q"},"source":["Now print your policy and make sure it leads to the upper-right corner which is the termnial state returning the most rewards."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8v2-4dBzay8Q","outputId":"25ae1ab1-1550-4459-cde7-e0552b568349"},"outputs":[{"name":"stdout","output_type":"stream","text":["final values:\n","---------------------------\n"," 0.89| 0.99| 1.10| 0.00|\n","---------------------------\n"," 0.80| 0.00| 0.99| 0.00|\n","---------------------------\n"," 0.72| 0.80| 0.89| 0.80|\n","final policy:\n","---------------------------\n","  R  |  R  |  R  | N/A |\n","---------------------------\n","  U  | N/A |  U  | N/A |\n","---------------------------\n","  U  |  R  |  U  |  L  |\n"]}],"source":["print(\"final values:\")\n","print_values(V, grid)\n","print(\"final policy:\")\n","print_policy(policy, grid)"]},{"cell_type":"markdown","metadata":{"id":"3aXAW7gHay8Q"},"source":["# "]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"provenance":[{"file_id":"https://github.com/lcharlin/80-629/blob/master/week12-MDPs/value_and_policy_iteration_solution.ipynb","timestamp":1677041075237}]}},"nbformat":4,"nbformat_minor":0}
