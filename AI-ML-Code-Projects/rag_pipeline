{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gdmWVpRhJW06bFCFfp37yiVf_CtjpL7H","timestamp":1751860462008}],"authorship_tag":"ABX9TyN6XPqBi08E76LpX/8UcI1r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"Ed-HgQfYObKH"}},{"cell_type":"code","source":["#Markdown Cell - Title"],"metadata":{"id":"YQ9lq_9oOjm6","executionInfo":{"status":"ok","timestamp":1751857525858,"user_tz":420,"elapsed":5,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# AskMyDocs – GPT + FAISS RAG Pipeline\n","#_A Retrieval-Augmented Generation project for internal document search + Q&A_\n"],"metadata":{"id":"YMowbBLqOaox","executionInfo":{"status":"ok","timestamp":1751857525865,"user_tz":420,"elapsed":6,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Code Cell – Install Dependencies"],"metadata":{"id":"fVoOqiOKPDB2","executionInfo":{"status":"ok","timestamp":1751857525913,"user_tz":420,"elapsed":47,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["!pip install openai PyPDF2 faiss-cpu sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gbTUxlJlPFdL","executionInfo":{"status":"ok","timestamp":1751857659610,"user_tz":420,"elapsed":133743,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"3baf21ed-b0ed-4416-af2a-31a634f48185"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.93.0)\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n","Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n","Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyPDF2, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed PyPDF2-3.0.1 faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvidia"]},"id":"10abcdc9724d446aaf2cc39e44ffae18"}},"metadata":{}}]},{"cell_type":"code","source":["#Markdown Cell – Load DOCX Documents"],"metadata":{"id":"XToTgF9ZPmlM","executionInfo":{"status":"ok","timestamp":1751857659617,"user_tz":420,"elapsed":2,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["## 📄 Step 1: Load.DOCX document\n","#We'll use PYTHON-DOCX to extract text from internal DOCXs."],"metadata":{"id":"9qyef6rXP3TW","executionInfo":{"status":"ok","timestamp":1751857659619,"user_tz":420,"elapsed":1,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["!pip install python-docx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NG6QfuvS6E7","executionInfo":{"status":"ok","timestamp":1751857674372,"user_tz":420,"elapsed":14748,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"08e1b34e-2bfb-4b2f-c1b9-686d453d60f1"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n","Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.0)\n","Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m225.3/253.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.2.0\n"]}]},{"cell_type":"code","source":["from docx import Document\n","\n","def load_docx_text(file_path):\n","    doc = Document(file_path)\n","    return \"\\n\".join([para.text for para in doc.paragraphs])\n","\n","# Use your file path (adjust if needed)\n","file_path = \"/content/sample_data/rag-development-project.docx\"\n","docx_text = load_docx_text(file_path)\n","\n","print(docx_text[:1000])  # Preview sample\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0sWBV_giTCet","executionInfo":{"status":"ok","timestamp":1751858082030,"user_tz":420,"elapsed":205,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"a3e8082f-1628-4bc0-ca4f-89f3d577ee41"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\n","\n","You said:\n","no ill wait... but i do want to create a project specifically for dovetail and post it privately just to john after the interview and invite him to co my channel...i want to incorporate allm of the rag tools we discussed and demo i i would be able to take a user requestb or project requestb from start to finish and implementation... sort of a mini project... can you think of an interesting way to shocase my technical skils in case they have doubts after just 15 min oftaking... there will be 2 people on conversation...probay someone that is project manager or technical lead\n","\n","\n","ChatGPT said:\n","That’s a brilliant idea — and exactly the kind of initiative that can turn a short 15-minute interview into a full-time opportunity. Here’s how you can structure it strategically to show off your technical and problem-solving skills in a compact, real-world way — tailored just for Dovetail.\n","\n","🔷 Goal:\n","Create a mini project that demonstrates:\n","\n","Your ability to build a RAG pipeline\n","\n","Use of GP\n"]}]},{"cell_type":"code","source":["#Markdown Cell – Step 2: Load Chunk Function"],"metadata":{"id":"0kOkY5VSRiaC","executionInfo":{"status":"aborted","timestamp":1751857674741,"user_tz":420,"elapsed":6,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2 – Chunk the docx text\n","def split_text_into_chunks(text, chunk_size=500, overlap=50):\n","    chunks = []\n","    start = 0\n","    while start < len(text):\n","        end = start + chunk_size\n","        chunks.append(text[start:end])\n","        start += chunk_size - overlap\n","    return chunks\n","\n","# Apply to your docx content\n","chunks = split_text_into_chunks(docx_text)\n","print(f\"{len(chunks)} chunks created.\")\n","print(chunks[0])  # Preview first chunk\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZxYNZTmWZz0","executionInfo":{"status":"ok","timestamp":1751858087743,"user_tz":420,"elapsed":7,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"d76e9d79-0cb0-407a-8d3b-53c2207ea03c"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["110 chunks created.\n","\n","\n","\n","\n","You said:\n","no ill wait... but i do want to create a project specifically for dovetail and post it privately just to john after the interview and invite him to co my channel...i want to incorporate allm of the rag tools we discussed and demo i i would be able to take a user requestb or project requestb from start to finish and implementation... sort of a mini project... can you think of an interesting way to shocase my technical skils in case they have doubts after just 15 min oftaking... ther\n"]}]},{"cell_type":"code","source":["#Step 3: Embedding the Chunks"],"metadata":{"id":"cMt3wpsbWx0J","executionInfo":{"status":"aborted","timestamp":1751857674749,"user_tz":420,"elapsed":148977,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sentence_transformers import SentenceTransformer\n","\n","def embed_text_chunks(chunks, model_name=\"all-MiniLM-L6-v2\"):\n","    model = SentenceTransformer(model_name)\n","    embeddings = model.encode(chunks, convert_to_numpy=True)\n","    return embeddings, model\n","\n","# Run embedding\n","embeddings, embed_model = embed_text_chunks(chunks)\n","print(f\"Generated {len(embeddings)} embeddings.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nbpqcICYW1KE","executionInfo":{"status":"ok","timestamp":1751858125809,"user_tz":420,"elapsed":31608,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"c8705320-b314-43ac-86ee-d4a05a73c750"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Generated 110 embeddings.\n"]}]},{"cell_type":"code","source":["#Step 4: Build the FAISS Index"],"metadata":{"id":"RdPniNCsYJmI","executionInfo":{"status":"aborted","timestamp":1751857674756,"user_tz":420,"elapsed":148983,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import faiss\n","import numpy as np\n","\n","def build_faiss_index(embeddings):\n","    dim = embeddings.shape[1]\n","    index = faiss.IndexFlatL2(dim)\n","    index.add(embeddings)\n","    return index\n","\n","# Build index\n","faiss_index = build_faiss_index(embeddings)\n","print(\"✅ FAISS index built!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y19nOwFgYNB0","executionInfo":{"status":"ok","timestamp":1751858136916,"user_tz":420,"elapsed":71,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"28acf64f-7924-4328-e922-79e750e82f60"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ FAISS index built!\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()\n"],"metadata":{"id":"dGfanY5ylk73","executionInfo":{"status":"aborted","timestamp":1751857674767,"user_tz":420,"elapsed":148992,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step 5: Ask a Question Using GPT + Retrieved Chunks"],"metadata":{"id":"SGuHvxv-YiSZ","executionInfo":{"status":"aborted","timestamp":1751857674776,"user_tz":420,"elapsed":149001,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from openai import OpenAI\n","\n","# Load and clean your OpenAI key from file\n","with open(\"/content/sample_data/openai-key.txt\", \"r\") as file:\n","    os.environ[\"OPENAI_API_KEY\"] = file.read().encode(\"utf-8\").decode(\"utf-8-sig\").strip()\n","\n","# Must instantiate client AFTER setting the environment variable\n","client = OpenAI()\n","\n","def search_faiss(query, model, index, chunks, top_k=3):\n","    query_vector = model.encode([query]).astype(\"float32\")\n","    distances, indices = index.search(query_vector, top_k)\n","    return [chunks[i] for i in indices[0]]\n","\n","def generate_answer(query, context_chunks):\n","    context = \"\\n\\n\".join(context_chunks)\n","    prompt = f\"\"\"Use the following context to answer the question.\n","\n","Context:\n","{context}\n","\n","Question:\n","{query}\n","\n","Answer:\"\"\"\n","\n","    response = client.chat.completions.create(\n","        model=\"gpt-4\",\n","        messages=[{\"role\": \"user\", \"content\": prompt}],\n","        temperature=0.3,\n","        max_tokens=500\n","    )\n","\n","    return response.choices[0].message.content\n"],"metadata":{"id":"v9bpMlAInvpt","executionInfo":{"status":"ok","timestamp":1751858156141,"user_tz":420,"elapsed":1688,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#1. Test the retrieval + generation pipeline end-to-end"],"metadata":{"id":"rAeR-O2Rif_t","executionInfo":{"status":"aborted","timestamp":1751857674799,"user_tz":420,"elapsed":149023,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from openai import OpenAI\n","\n","# Assuming your API key was properly loaded in Step 5:\n","client = OpenAI()\n"],"metadata":{"id":"KfhxqULX5okB","executionInfo":{"status":"ok","timestamp":1751858455623,"user_tz":420,"elapsed":59,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def generate_answer(query, context_chunks):\n","    context = \"\\n\\n\".join(context_chunks)\n","    print(f\"[MOCK GPT CALL] Using context:\\n{context}\\n\")\n","    return f\"(Simulated GPT response to your question: '{query}')\"\n"],"metadata":{"id":"wJq7VnfV51WX","executionInfo":{"status":"ok","timestamp":1751858506395,"user_tz":420,"elapsed":43,"user":{"displayName":"Loretta","userId":"14548570598164544149"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["query = \"What is this RAG project supposed to demonstrate?\"\n","top_chunks = search_faiss(query, embed_model, faiss_index, chunks)\n","final_answer = generate_answer(query, top_chunks)\n","\n","print(final_answer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eLYN-t22pQNL","executionInfo":{"status":"ok","timestamp":1751858531206,"user_tz":420,"elapsed":26,"user":{"displayName":"Loretta","userId":"14548570598164544149"}},"outputId":"78c1762c-07f5-4863-b4ab-b62ad43ad8bb"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[MOCK GPT CALL] Using context:\n","summary for each of the step one processes and explain why used... lets start from here to explain to john or anyone why/what/RAG installed these libries, more than -short description... it will later be used as a 'guide' to help other contract developers folllow a structured approch to project deveopment... this will be sort of a 'users manual' for desigining client requests\n","\n","\n","ChatGPT said:\n","Perfect — here’s a clear, structured summary of Step 1: RAG Environment Setup, written like a developer’s\n","\n","RAG assistant\n","\n","Created by: Loretta Gray\n","GitHub: Loretta991\n","Slack Channel: #ellegreyllc-rag-developer\n","\n","yaml\n","Copy\n","Edit\n","\n","---\n","\n","## 📦 `requirements.txt`\n","\n","```txt\n","openai\n","PyPDF2\n","faiss-cpu\n","sentence-transformers\n","numpy\n","💬 What You Can Say in the Interview:\n","“I’ve structured my RAG solution as a standalone project with modular Python files, a single master notebook, and a README that any engineer or contractor could follow. This ensures clarity when collaborating with teams or integrating with larger architect\n","\n","b-based RAG mini-project, tailored to Dovetail and future clients — showing that you can:\n","\n","✅ Understand RAG architecture\n","✅ Build and explain each part step-by-step\n","✅ Use logical prompting + documentation\n","✅ Ship something that feels real and is repeatable\n","\n","✅ Here's What I’ll Build for You (handheld + modular)\n","Project Name: AskMyDocs-RAG\n","\n","Platform: Google Colab & Jupyter Notebook\n","Target Audience: Hiring team at Dovetail + future clients/employers\n","Experience Level: Beginner-friendly, explain-like-I\n","\n","(Simulated GPT response to your question: 'What is this RAG project supposed to demonstrate?')\n"]}]}]}